{"meta":{"title":"哟，一个大面团","subtitle":null,"description":"太阳那么红，为什么还那么冷","author":"PG-55","url":"http://damiantuan.xyz"},"pages":[{"title":"说个笑话","date":"2017-11-15T06:04:44.000Z","updated":"2017-11-18T06:35:32.514Z","comments":true,"path":"about/index.html","permalink":"http://damiantuan.xyz/about/index.html","excerpt":"","text":"关于自己人究竟是理性的还是非理性的？经济学说人应该都是理性的人，而经济行为学却告诉我们人出于理性的行为是多么荒诞不经。管理学说做管理应该是理性的，而世界上的管理却有很多用屁股代替脑袋的事情。运筹学说如果一个整体的所有因素都是最佳的，那么这个整体将呈现最佳状态，而博弈论告诉我们囚徒困境。 “和其他人不一样的话，我会被笑话的”“所以我们都活成了笑话” 于是我带上面具为人们带来了笑话，结果到最后发现自己就是个笑话。"},{"title":"分类","date":"2017-11-16T07:07:10.000Z","updated":"2017-11-19T08:00:57.235Z","comments":true,"path":"categories/index.html","permalink":"http://damiantuan.xyz/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-11-15T06:54:21.000Z","updated":"2017-11-15T07:39:31.140Z","comments":true,"path":"tags/index.html","permalink":"http://damiantuan.xyz/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hello World","slug":"hello-world","date":"2018-02-20T13:28:46.067Z","updated":"2017-11-18T07:52:41.738Z","comments":true,"path":"2018/02/20/hello-world/","link":"","permalink":"http://damiantuan.xyz/2018/02/20/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Startjust testCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"浅谈HTTP中Get与Post","slug":"浅谈HTTP中Get与Post","date":"2017-12-09T06:25:55.000Z","updated":"2017-12-09T08:13:31.272Z","comments":true,"path":"2017/12/09/浅谈HTTP中Get与Post/","link":"","permalink":"http://damiantuan.xyz/2017/12/09/浅谈HTTP中Get与Post/","excerpt":"前面的一部分是自己参考网上的一些比较浅显易懂的文章写的，也引用了一些段子；第二部分就是正儿八经的搞知识了。","text":"前面的一部分是自己参考网上的一些比较浅显易懂的文章写的，也引用了一些段子；第二部分就是正儿八经的搞知识了。 装逼部分GET和POST是HTTP请求的两种基本方法，要说它们的区别，接触过WEB开发的人都能说出一二。最直观的区别就是GET把参数包含在URL中，POST通过request body传递参数。其实网络传输方法有很多种，一个表格可以全部概述。 HTTPMethod RFC Request Has Body Response Has Body safe Idempotent Cacheable GET RFC7231 NO YES YES YES YES HEAD RFC7231 NO NO YES YES YES POST RFC7231 YES YES NO NO YES PUT RFC7231 YES YES NO YES NO DELETE RFC7231 NO YES NO YES NO CONNECT RFC7231 YES YES NO NO NO OPTIONS RFC7231 Optional YES YES YES NO TRACE RFC7231 NO YES YES YES NO PATCH RFC5789 YES YES NO NO YES 关于POST方法与GET方法的区别，很多教科书或是计算机类的书籍都会像下面这样写，恩，大致意思就是这样 GET在浏览器回退时是无害的，而POST会再次提交请求。 GET产生的URL地址可以被Bookmark，而POST不可以。 GET请求会被浏览器主动cache，而POST不会，除非手动设置。 GET请求只能进行url编码，而POST支持多种编码方式。 GET请求参数会被完整保留在浏览器历史记录里，而POST中的参数不会被保留。 GET请求在URL中传送的参数是有长度限制的，而POST么有。 对参数的数据类型，GET只接受ASCII字符，而POST没有限制。 GET比POST更不安全，因为参数直接暴露在URL上，所以不能用来传递敏感信息。 GET参数通过URL传递，POST放在Request body中。 然而这并不具有技术上的普适性，甚至有些说法还是错误的，比如说URL并没有长度的限制，URL长度的限制是与浏览器与系统有关的，其中所说的系统包括终端系统和服务端系统，URl在产生的时候其本身并没有任何的限制；再者来说安全性的问题，我只能说不考虑应用情景的所谓安全性都是在耍流氓。 首先我们要明白什么GET和POST。答案：HTTP协议中的两种发送请求的方法。说明白点，就是通信方式。比如A要联系B，那么A可以选择打电话或是发短信，微信和QQ，还可以写信，这其中不管那种方法都可以联系都B，只不过是形式不一样。HTTP是什么？答案：HTTP（超文本传输协议（HTTP，HyperText Transfer Protocol)）是基于TCP/IP的关于数据如何在万维网中如何通信的协议。这个有点像使用说明。互联网上面两台计算机互不相识，谁也不知道谁，HTTP中说明了一系列的东西，协议版本号，接受的字符类型，客户信息和内容之类的。就像你接到一分书信，一开始写明了是用中文书写的，你要找会中文的人，然后发现这种中文隶书写的，你还要从会中文的人中筛选出一个能看隶书的人来，然后······经过一系列的努力，这样到最后就可以解读出这份书信了。TCP/IP是什么？答案：Transmission Control Protocol/Internet Protocol的简写，中译名为传输控制协议/因特网互联协议，又名网络通讯协议，是Internet最基本的协议、Internet国际互联网络的基础，由网络层的IP协议和传输层的TCP协议组成。TCP/IP 定义了电子设备如何连入因特网，以及数据如何在它们之间传输的标准。协议采用了4层的层级结构，每一层都呼叫它的下一层所提供的协议来完成自己的需求。再说明白点，TCP/IP就是规定了互联网中计算机如何通信的协议，就像写信的时候是邮局告诉你规定邮政编码一样。 这样看，我们知道HTTP的底层是TCP/IP。所以GET和POST的底层也是TCP/IP，也就是说，GET/POST都是TCP链接。GET和POST能做的事情是一样一样的。你要给GET加上request body，给POST带上url参数，技术上是完全行的通的。 在万维网世界中，TCP就像汽车，我们用TCP来运输数据，它很可靠，从来不会发生丢件少件的现象。但是如果路上跑的全是看起来一模一样的汽车，那这个世界看起来是一团混乱，送急件的汽车可能被前面满载货物的汽车拦堵在路上，整个交通系统一定会瘫痪。为了避免这种情况发生，交通规则HTTP诞生了。HTTP给汽车运输设定了好几个服务类别，有GET, POST, PUT, DELETE等等，HTTP规定，当执行GET请求的时候，要给汽车贴上GET的标签(设置method为GET)，而且要求把传送的数据放在车顶上(url中)以方便记录。如果是POST请求，就要在车上贴上POST的标签，并把货物放在车厢里。当然，你也可以在GET的时候往车厢内偷偷藏点货物，但是这是很不光彩;也可以在POST的时候在车顶上也放一些数据，让人觉得傻乎乎的。HTTP只是个行为准则，而TCP才是GET和POST怎么实现的基本。 GET和POST还有一个重大区别，简单的说：GET产生一个TCP数据包;POST产生两个TCP数据包。长的说：对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200(返回数据);而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok(返回数据)。也就是说，GET只需要汽车跑一趟就把货送到了，而POST得跑两趟，第一趟，先去和服务器打个招呼“嗨，我等下要送一批货来，你们打开门迎接我”，然后再回头把货送过去。因为POST需要两步，时间上消耗的要多一点，看起来GET比POST更有效。因此Yahoo团队有推荐用GET替换POST来优化网站性能。但这是一个坑!跳入需谨慎。为什么? GET与POST都有自己的语义，不能随便混用。 据研究，在网络环境好的情况下，发一次包的时间和发两次包的时间差别基本可以无视。而在网络环境差的情况下，两次包的TCP在验证数据包完整性上，有非常大的优点。 并不是所有浏览器都会在POST中发送两次包，Firefox就只发送一次。很好，这就是装逼的第一步，完成了。 不装逼部分 第二部分引用陈曦明的文章，出于对知识的敬畏，不装逼了，认认真真搞。 Http定义了与服务器交互的不同方法，最基本的方法有4种，分别是GET，POST，PUT，DELETE。URL全称是资源描述符，我们可以这样认为：一个URL地址，它用于描述一个网络上的资源，而HTTP中的GET，POST，PUT，DELETE就对应着对这个资源的查，改，增，删4个操作。到这里，大家应该有个大概的了解了，GET一般用于获取/查询资源信息，而POST一般用于更新资源信息。 1.根据HTTP规范，GET用于信息获取，而且应该是安全的和幂等的。 (1).所谓安全的意味着该操作用于获取信息而非修改信息。换句话说，GET 请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。 * 注意：这里安全的含义仅仅是指是非修改信息。 (2).幂等的意味着对同一URL的多个请求应该返回同样的结果。这里我再解释一下幂等这个概念： 幂等（idempotent、idempotence）是一个数学或计算机学概念，常见于抽象代数中。 幂等有一下几种定义： 对于单目运算，如果一个运算对于在范围内的所有的一个数多次进行该运算所得的结果和进行一次该运算所得的结果是一样的，那么我们就称该运算是幂等的。比如绝对值运算就是一个例子，在实数集中，有abs(a)=abs(abs(a))。 对于双目运算，则要求当参与运算的两个值是等值的情况下，如果满足运算结果与参与运算的两个值相等，则称该运算幂等，如求两个数的最大值的函数，有在在实数集中幂等，即max(x,x) = x。看完上述解释后，应该可以理解GET幂等的含义了。 但在实际应用中，以上2条规定并没有这么严格。引用别人文章的例子：比如，新闻站点的头版不断更新。虽然第二次请求会返回不同的一批新闻，该操作仍然被认为是安全的和幂等的，因为它总是返回当前的新闻。从根本上说，如果目标是当用户打开一个链接时，他可以确信从自身的角度来看没有改变资源即可。 2.根据HTTP规范，POST表示可能修改变服务器上的资源的请求。继续引用上面的例子：还是新闻以网站为例，读者对新闻发表自己的评论应该通过POST实现，因为在评论提交后站点的资源已经不同了，或者说资源被修改了。 上面大概说了一下HTTP规范中GET和POST的一些原理性的问题。但在实际的做的时候，很多人却没有按照HTTP规范去做，导致这个问题的原因有很多，比如说： 1.很多人贪方便，更新资源时用了GET，因为用POST必须要到FORM（表单），这样会麻烦一点。 2.对资源的增，删，改，查操作，其实都可以通过GET/POST完成，不需要用到PUT和DELETE。 3.另外一个是，早期的Web MVC框架设计者们并没有有意识地将URL当作抽象的资源来看待和设计，所以导致一个比较严重的问题是传统的Web MVC框架基本上都只支持GET和POST两种HTTP方法，而不支持PUT和DELETE方法。 * 简单解释一下MVC：MVC本来是存在于Desktop程序中的，M是指数据模型，V是指用户界面，C则是控制器。使用MVC的目的是将M和V的实现代码分离，从而使同一个程序可以使用不同的表现形式。 以上3点典型地描述了老一套的风格（没有严格遵守HTTP规范），随着架构的发展，现在出现REST(Representational State Transfer)，一套支持HTTP规范的新风格，这里不多说了，可以参考《RESTful Web Services》。 说完原理性的问题，我们再从表面现像上面看看GET和POST的区别： 1.GET请求的数据会附在URL之后（就是把数据放置在HTTP协议头中），以?分割URL和传输数据，参数之间以&amp;相连，如：login.action?name=hyddd&amp;password=idontknow&amp;verify=%E4%BD%A0%E5%A5%BD。如果数据是英文字母/数字，原样发送，如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64加密，得出如：%E4%BD%A0%E5%A5%BD，其中％XX中的XX为该符号以16进制表示的ASCII。 POST把提交的数据则放置在是HTTP包的包体中。 2.”GET方式提交的数据最多只能是1024字节，理论上POST没有限制，可传较大量的数据，IIS4中最大为80KB，IIS5中为100KB”？？！ 以上这句是我从其他文章转过来的，其实这样说是错误的，不准确的： (1).首先是”GET方式提交的数据最多只能是1024字节”，因为GET是通过URL提交数据，那么GET可提交的数据量就跟URL的长度有直接关系了。而实际上，URL不存在参数上限的问题，HTTP协议规范没有对URL长度进行限制。这个限制是特定的浏览器及服务器对它的限制。IE对URL长度的限制是2083字节(2K+35)。对于其他浏览器，如Netscape、FireFox等，理论上没有长度限制，其限制取决于操作系统的支持。 注意这是限制是整个URL长度，而不仅仅是你的参数值数据长度。[见参考资料5] (2).理论上讲，POST是没有大小限制的，HTTP协议规范也没有进行大小限制，说“POST数据量存在80K/100K的大小限制”是不准确的，POST数据是没有限制的，起限制作用的是服务器的处理程序的处理能力。 对于ASP程序，Request对象处理每个表单域时存在100K的数据长度限制。但如果使用Request.BinaryRead则没有这个限制。 由这个延伸出去，对于IIS 6.0，微软出于安全考虑，加大了限制。我们还需要注意： 1).IIS 6.0默认ASP POST数据量最大为200KB，每个表单域限制是100KB。 2).IIS 6.0默认上传文件的最大大小是4MB。 3).IIS 6.0默认最大请求头是16KB。 IIS 6.0之前没有这些限制。[见参考资料5] 所以上面的80K，100K可能只是默认值而已(注：关于IIS4和IIS5的参数，我还没有确认)，但肯定是可以自己设置的。由于每个版本的IIS对这些参数的默认值都不一样，具体请参考相关的IIS配置文档。 3.在ASP中，服务端获取GET请求参数用Request.QueryString，获取POST请求参数用Request.Form。在JSP中，用request.getParameter(\\”XXXX\\”)来获取，虽然jsp中也有request.getQueryString()方法，但使用起来比较麻烦，比如：传一个test.jsp?name=hyddd&amp;password=hyddd，用request.getQueryString()得到的是：name=hyddd&amp;password=hyddd。在PHP中，可以用\\$_GET和\\$_POST分别获取GET和POST中的数据，而\\$_REQUEST则可以获取GET和POST两种请求中的数据。值得注意的是，JSP中使用request和PHP中使用$_REQUEST都会有隐患，这个下次再写个文章总结。 4.POST的安全性要比GET的安全性高。注意：这里所说的安全性和上面GET提到的“安全”不是同个概念。上面“安全”的含义仅仅是不作数据修改，而这里安全的含义是真正的Security的含义，比如：通过GET提交数据，用户名和密码将明文出现在URL上，因为(1)登录页面有可能被浏览器缓存，(2)其他人查看浏览器的历史纪录，那么别人就可以拿到你的账号和密码了，除此之外，使用GET提交数据还可能会造成Cross-site request forgery攻击。 总结一下，Get是向服务器发索取数据的一种请求，而Post是向服务器提交数据的一种请求，在FORM（表单）中，Method默认为”GET”，实质上，GET和POST只是发送机制不同，并不是一个取一个发！ 参考资料： [1].http://hi.baidu.com/liuzd003/blog/item/7bfecbfa6ea94ed8b58f318c.html [2].http://www.blogjava.net/onlykeke/archive/2006/08/23/65285.aspx [3].http://baike.baidu.com/view/2067025.htm [4].http://www.chxwei.com/article.asp?id=373 [5].http://blog.csdn.net/somat/archive/2004/10/29/158707.aspx 引用地址：https://www.cnblogs.com/hyddd/archive/2009/03/31/1426026.html","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://damiantuan.xyz/categories/爬虫/"}],"tags":[{"name":"网络","slug":"网络","permalink":"http://damiantuan.xyz/tags/网络/"}]},{"title":"网页状态码及其处理","slug":"网页状态码及其处理","date":"2017-12-03T09:55:21.000Z","updated":"2017-12-03T10:03:35.516Z","comments":true,"path":"2017/12/03/网页状态码及其处理/","link":"","permalink":"http://damiantuan.xyz/2017/12/03/网页状态码及其处理/","excerpt":"记录一下在学习爬虫过程中遇到的几种网页的状态码以及处理方法","text":"记录一下在学习爬虫过程中遇到的几种网页的状态码以及处理方法 写几个主要的，像200这样访问成功的状态码就没有必要写下来了。 状态码及其含义 400 Bad Request 客户端请求有语法错误，不能被服务器所理解 401 Unauthorized 请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden 服务器收到请求，但是拒绝提供服务 404 Not Found 请求资源不存在，eg：输入了错误的URL 500 Internal Server Error 服务器发生不可预期的错误 503 Server Unavailable 服务器当前不能处理客户端的请求，一段时间后可能恢复正常 处理方法400 Bad Request - 检查请求的参数或者路径 401 Unauthorized - 如果需要授权的网页，尝试重新登录 403 Forbidden 如果是需要登录的网站，尝试重新登录 IP被封，暂停爬取，并增加爬虫的等待时间，如果拨号网络，尝试重新联网更改IP 404 - Not Found 直接丢弃5XX - 服务器错误，直接丢弃，并计数，如果连续不成功，WARNING 并停止爬取","categories":[{"name":"python,爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/categories/python-爬虫/"}],"tags":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/tags/python爬虫/"}]},{"title":"计算上证日K与个股的相关性系数","slug":"计算上证日K与个股的相关性系数","date":"2017-11-21T07:06:05.000Z","updated":"2017-11-21T08:30:10.476Z","comments":true,"path":"2017/11/21/计算上证日K与个股的相关性系数/","link":"","permalink":"http://damiantuan.xyz/2017/11/21/计算上证日K与个股的相关性系数/","excerpt":"这算是一个面试的题吧。需求是把与上证日K趋势的相关性系数大于80%的个股挑选出来。","text":"这算是一个面试的题吧。需求是把与上证日K趋势的相关性系数大于80%的个股挑选出来。这是一个面试题，当时面试一个做量化交易的公司的一个面试题，不过对方对我这个脚本似乎不太满意，面试的后续就没有后续了。 思路数据获取 在上交所正常盘面交易的股票大约有3400多只，如果写爬虫去获取每一只股票的相关信息那么工程量肯定不小，当然加上现在各个金融网站的反爬手段，单单写爬虫这点可能都会卡住，就在网上看看有没有什么免费的API可以使用，无意间找到了tushare这个模块，全中文手册，使用全免费，这个当然是不错的。后面在使用的过程中发现在获取当日交易情况的时候会出现不稳定然后中断获取的情况，这个······毕竟免费的而且没有像各个官方的限制，其实还是很不错的，而且也提供了大量的方法可以使用。 数据处理 在获取完数据之后应该对数据进行处理，我查看了一下tushare输出的数据类型，基本上都是DataFormat的类型，这点处理上是比较有利的，但是考虑到tushare在获取数据的时候的稳定性情况，我决定不直接使用网络数据，转而使用把数据保存到本地后再使用，在使用完成后我们可以对本地文件进行删除。在这里我不考虑对每一项数据都进行入库处理，因为tushare提供了一个将数据保存的本地的方法，使用起来也十分方便。 提取结果 每只股票的代码是6位数，相关性系数检验在80%以上的应该不会很多，所以我采用直接将股票代码保存在一个txt文本文件中。 方法获取有效股票代码 选择使用tushre之后，知道了如何获取个股的数据和上证指数，但是模块中没有提供我想要的单单的股票代码，我也不能用循环去生成股票代码，虽然说股票的代码确实是有顺序的，但是不敢保证生成的股票代码的那只股票现在是否还在市。想想，如果可以获取当日的交易的所有股票行情的话这个问题不就解决了，因为当日还在交易的所有股票肯定还没有退市的。查一下手册，发现有一个函数是一次性获取当前交易所有股票的行情数据，ok就是他了。获取到了之后保存为本地excel表格，再从表格中读取股票代码。 计算K线趋势 后面发现使用tushare获取的数据并没有计算出当日的K线的走势，不过没关系了，他给出的数据中有一天的开盘价，收盘价，最高价和最低价，这些足够计算当日的K线了。正值为红，负值为绿。在计算的过程中发现读出来的数据是一个列表，其中包括表头的值，于是取下标从1开始。列表的计算在python并不是很方便，因为python并没有像c一样提供一个数组的数据结构，幸好前几天刚刚学习了numpy模块，现学现卖了，把读取出来的列表通过numpy模块转换成数组，再使用numpy模块内置算法计算，这样大大的减少了工作量。 相关性系数 Pearson系数也称为是简单相关性系数，但是计算中要求两个计算的量近似服从二维的正态分布，而上证K线的趋势和个股K线的趋势其实并没有什么规律，不能使用这个，于是采用Spearman系数，Spearman系数在计算的过程中并不要求计算的两组数字的分布规律。 工作流程 工作流程就是获取上证指数的信息，保存为表格，读表，计算K线，获取当日交易情况，保存为表格，读表，处理，得出个股编号，再通过编号去获取该股票的历史数据，保存为表格，读表，计算K线，再把个股的K线与上证的K线从获取之日开始计算，得出Spearman系数，最后把大于Spearman系数大于80%的个股编号保存下来，不满足条件的直接忽略。最后计算完该股票后把保存该股票信息的excel表格删掉，减少占用磁盘空间。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# *-*coding='UTF-8'*-*import tushare as tsimport numpy as npimport xlrdimport pandasimport timeimport os# 获取上证指数的K线def getshangzheng_K(): #读取上证指数的日K shangzheng = ts.get_hist_data('sh',ktype='D',start='2017-07-01') # 将上证指数的日K存入excel表格 shangzheng.to_excel('sh.xlsx',sheet_name='Sheet1') (filepath,tempfilename)=os.path.split('sh.xlsx') return tempfilename# 读取excel表格def read_excel(fileName='',sheet='Sheet1'): workbook=xlrd.open_workbook(fileName) sheet1=workbook.sheet_by_name(sheet) return sheet1# 计算K线的趋势def K(sheet,open_col_values=False,close_col_values=False): open_price=sheet.col_values(open_col_values) close_price=sheet.col_values(close_col_values) open_price_array=np.array(open_price[1:],dtype=float) close_price_array=np.array(close_price[1:],dtype=float) movements=close_price_array-open_price_array the_k=pandas.Series(movements) return the_k# 获取上证交易所中当日正在交易的所有股票信息def get_today_stock(): todayall=ts.get_today_all() todayall.to_excel('todays.xlsx') (filepath,tempfilename)=os.path.split('todays.xlsx') return tempfilename# 获取上证中正在交易的股票的代码def get_stock_num(sheet=''): todaycols=sheet.col_values(1) return todaycols# 获取个股的详细信息def get_stock_info(stock_num): stock_number=str(stock_num) stock_info=ts.get_hist_data(stock_number,start='2017-07-01',ktype='D') stock_info.to_excel(stock_number+'.xlsx') (filepath,tempfilename)=os.path.split(stock_number+'.xlsx') return tempfilename# 计算相关性系数，因为数据分布无规则，所以计算spearman相关系数def find_spearman(sh,stock): corr_spearman=sh.corr(stock,method='spearman') return corr_spearmanif __name__=='__main__': #获取上证信息并保存为表格 sh_excel=getshangzheng_K() #读取上证信息表格 sh_sheet=read_excel(sh_excel,sheet='Sheet1') # 计算上证日K的趋势 sh_K=K(sh_sheet,open_col_values=1,close_col_values=3) # 计算个股的日K趋势 today_stock_exccel=get_today_stock() #实际使用中，read_excel函数的第一个参数可以直接写成文件名，这样不必每次执行都重复去获取数据 today_sheet=read_excel(today_stock_exccel,sheet='Sheet1') stock_nums=get_stock_num(today_sheet) for stock_num in stock_nums[1:]: stock_excel=get_stock_info(stock_num) stock_sheet=read_excel(fileName=stock_excel,sheet='Sheet1') stock_K=K(sheet=stock_sheet,open_col_values=1,close_col_values=3) #以上为计算个股的K线趋势 #此处计算Spearman系数 spearman=find_spearman(sh_K,stock_K) if spearman&gt;0.800: with open('save.txt','a+') as fp: fp.write(str(stock_num).encode('utf8') + '\\t') print 'done one',time.strftime(\"%Y-%m-%d %X\", time.localtime()) else: print 'let one go',time.strftime(\"%Y-%m-%d %X\", time.localtime()) #这句为删除使用过的表格，如果想保留相关表格，可以将其注释掉 os.remove(str(stock_num)+'.xlsx') 在实际使用的过程中我们可以将上证的表格和当日成交的表格给保留下来，并在代码中注释掉相应的执行语句，将文件名直接填成参数，这样可以避免获取数据的时候tushare的不稳定造成的影响。 最后，计算的结果为300719 300716 002781 000555","categories":[{"name":"量化分析","slug":"量化分析","permalink":"http://damiantuan.xyz/categories/量化分析/"}],"tags":[{"name":"-python -量化分析","slug":"python-量化分析","permalink":"http://damiantuan.xyz/tags/python-量化分析/"}]},{"title":"scrapy爬取当当网所有3c产品商品名称，价格，评论数","slug":"scrapy爬取当当网所有3c产品商品名称，价格，评论数","date":"2017-11-19T08:31:19.000Z","updated":"2017-11-19T09:38:31.784Z","comments":true,"path":"2017/11/19/scrapy爬取当当网所有3c产品商品名称，价格，评论数/","link":"","permalink":"http://damiantuan.xyz/2017/11/19/scrapy爬取当当网所有3c产品商品名称，价格，评论数/","excerpt":"Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。Scrapy 使用 Twisted这个异步网络库来处理网络通讯，架构清晰，并且包含了各种中间件接口，可以灵活的完成各种需求。","text":"Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。Scrapy 使用 Twisted这个异步网络库来处理网络通讯，架构清晰，并且包含了各种中间件接口，可以灵活的完成各种需求。 Scrapy 首先祭上scrapy的工作流程图。 我们暂时不去管scrapy的设计，先看看怎么用先。 在命令终端输入123scrapy startproject dangdangcd dangdangscrapy genspider dangdangwang dangdang.com 解释一下上面的命令的含义，startporject是新建一个工程，cd就是到新建的工程项目文件夹下，genspider是依照模板生成一个爬虫，默认状态是basic模板。 在工程文件夹下的终端输入tree就可以得到如下的文件，如果输入tree显示找不到该命令的话，先安装一个小软件，apt-get install tree ,附上tree的手册tree(1) - Linux man page1234567891011121314151617181920tree .├── 201708260.txt├── dangdang│ ├── __init__.py│ ├── __init__.pyc│ ├── items.py│ ├── items.pyc│ ├── middlewares.py│ ├── pipelines2excel.py│ ├── pipelines.py│ ├── pipelines.pyc│ ├── settings.py│ ├── settings.pyc│ └── spiders│ ├── dangdangwang.py│ ├── dangdangwang.pyc│ ├── __init__.py│ └── __init__.pyc└── scrapy.cfg 文件布局如上面树所示 开始工作修改item.py文件 item.py是定义scrapy抓取信息的地方，相当与你在这里要为你想得到的每一种信息都要取一个名字，名字当然是见字知意最好了。 12345678910111213141516171819# -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# http://doc.scrapy.org/en/latest/topics/items.htmlimport scrapyclass DangdangItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 商品名称 title=scrapy.Field() # 评论数 num=scrapy.Field() # 价格 price=scrapy.Field() # 页面链接 link=scrapy.Field() 修改dangdangwang.py文件 dangdangwang.py文件位于spiders文件夹下，是爬虫的主要行为，他定义了如何去定位信息，把什么信息赋值给哪个item。 123456789101112131415161718192021222324252627# -*- coding: utf-8 -*-import scrapyfrom dangdang.items import DangdangItemclass DangdangwangSpider(scrapy.Spider): name = \"dangdangwang\" allowed_domains = [\"dangdang.com\"] start_urls = ['http://category.dangdang.com/pg1-cid4002590.html'] def parse(self, response): # 每一个商品的点击链接（详情链接） url_list=response.xpath('.//*[@id=\"component_0__0__8609\"]/li/p[@class=\"name\"]/a/@href').extract() for url in url_list: yield scrapy.http.Request(url,callback=self.parse_name) # 搜索分类的分页 for i in range(2,98): page_url='http://category.dangdang.com/pg&#123;&#125;-cid4002590.html'.format(i) yield scrapy.http.Request(page_url,callback=self.parse) def parse_name(self,response): items=DangdangItem() items['title']=response.xpath('//div[@class=\"name_info\"]/h1/text()').extract() items['num']=response.xpath('//div[@class=\"pinglun\"]/a/text()').extract() items['price']=response.xpath('//div[@class=\"price_d\"]/p[@id=\"dd-price\"]/text()').extract() items['link']=response.url yield items 修改pipelines.py pipelines.py文件是定义了爬取的数据如何保存的文件，就像他的名字一样，数据的管道。 123456789101112131415161718192021222324# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.htmlimport timeimport os.pathclass DangdangPipeline(object): def process_item(self, item, spider): # print u\"商品名字：\"+item['title'][0] # print u\"商品价格：\"+item['price'][0] # print u\"商品评论数\"+item['num'][0] # return item today = time.strftime('%Y%m%d', time.localtime()) fileName = today + '.txt' with open(fileName,'a') as fp: fp.write(item['title'][0].replace(' ','').encode('utf8') + '\\t') fp.write(\"价格\"+item['price'][0].encode('utf8') + '\\t') fp.write(\"评论数\"+item['num'][0].encode('utf8') + '\\t') fp.write('\\n') # time.sleep(1) return item 这里定义的是将数据保存到一个txt记事本。效果如图所示","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"-python -爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"py+bs+requests多线程爬虫","slug":"py-bs-requests多线程爬虫","date":"2017-11-19T08:09:30.000Z","updated":"2017-11-19T08:15:21.921Z","comments":true,"path":"2017/11/19/py-bs-requests多线程爬虫/","link":"","permalink":"http://damiantuan.xyz/2017/11/19/py-bs-requests多线程爬虫/","excerpt":"还是那个站，不过我们这次不再一个一个来爬取了，我们将采用多线程进行爬取。","text":"还是那个站，不过我们这次不再一个一个来爬取了，我们将采用多线程进行爬取。 python的多线程问题一直是一个备受争议的话题，因为在多核心CPU的硬件条件下，python依然还是只能利用单核心，所以python的多线程就有一个“伪多线程”的命题，但是python多线程虽然是只能利用单核心，但是依旧比单线程要快。 思路还是一样，不过在最后处理的时候我们引入了一个多线程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#-*-coding:utf8-*-#!usr/bin/pythonimport requestsimport timefrom multiprocessing.dummy import Pool as ThreadPoolfrom bs4 import BeautifulSoupheader = &#123;'User-Agent':'\"Mozilla/5.0 (X11; Linux x86_64; rv:45.0) Gecko/20100101 Firefox/45.0\"'&#125;# 读取网页源码def getHtml(url): htmls=requests.get(url,headers=header) # 发现网页是用GBK编码的，在此处进行转码 htmls.encoding = 'gb2312' # 调用text将对象进行字符化 pageContent=htmls.text return pageContent# 进行文章url的获取def getContentUrl(html): urls=[] bsContent=BeautifulSoup(html,'html.parser') urlContent=bsContent.find(class_=\"liszw\") for link in urlContent.find_all('a'): url_lib=link.get('href') urls.append(url_lib) return urls#文章内容的获取def readContent(urls): articleHtml=getHtml(urls) # print articleHtml bsContent=BeautifulSoup(articleHtml,'html.parser') title=bsContent.find('h2').string content=bsContent.find(class_=\"arwzks\") article=content.get_text() txt=article.encode('utf-8') print title+' start' open(r'/home/wukong/testTuiLiXue/download/'+title.encode(\"utf-8\")+'.txt','w+').write(txt) print time.strftime('%Y-%m-%d %X',time.localtime(time.time()))+title+' end' # print txt return txtif __name__ == '__main__': #后面的参数为CPU的核心数，虽然说只能利用单核心 pool = ThreadPool(4) links=[] for i in range(1,40): links.append('http://tuilixue.com/zhentantuilizhishi/list_4_'+str(i)+'.html') for link in links: html=getHtml(link) urls=getContentUrl(html) url=[] for i in range(0,20,2): url.append(urls[i]) # for i in url: # result = readContent(i) result=pool.map(readContent,url) pool.close() pool.join()","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"-python -爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"BeautifulSoup爬虫代码","slug":"BeautifulSoup爬虫代码","date":"2017-11-19T07:40:14.000Z","updated":"2017-11-19T08:01:54.245Z","comments":true,"path":"2017/11/19/BeautifulSoup爬虫代码/","link":"","permalink":"http://damiantuan.xyz/2017/11/19/BeautifulSoup爬虫代码/","excerpt":"还是上次的那个网站，不过这次我们用request+beautifulsoup来进行爬取了。","text":"还是上次的那个网站，不过这次我们用request+beautifulsoup来进行爬取了。 思路和上次的那个基本上是一样的，不过就是把定位信息的方法从原来的使用python内置的str模块中的函数方法改成了使用beautifulsoup这个第三方的模块，这个模块的手册在网上能找到，翻译得不错，基本上是一看就懂的那种。注释的话我这次没写了，因为和上次一样的，想看注释的可以去看上一篇文章。 这次的存储方法与上次使用urllib的有所不同，上次的存储是直接保存HTML文件的要使用一些处理结构性文档的工具才能查看文章的内容，而且文件命名也是使用的网站上的URL来进行的，这样的命名毫无意义也就无法知道文件中的内容是什么，所以这次我们把爬取的文章标题作为文件名，保存为txt记事本文件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/usr/bin/python#coding=utf-8import requestsimport timefrom bs4 import BeautifulSoupurl=['']*20header = &#123;'User-Agent':'\"Mozilla/5.0 (X11; Linux x86_64; rv:45.0) Gecko/20100101 Firefox/45.0\"'&#125;for page in range(1,39): htmls=requests.get('http://tuilixue.com/zhentantuilizhishi/list_4_'+str(page)+'.html',headers=header) htmls.encoding = 'gb2312' pageContent=htmls.text txt=pageContent# print type(txt)# open(r'/home/wukong/testTuiLiXue/download/123.txt','w+').write(txt.encode('utf-8'))# print content bsContent=BeautifulSoup(pageContent,'html.parser') urlContent=bsContent.find(class_=\"liszw\") lis=urlContent.find_all('a') lis=str(lis) hrefHeader=lis.find(r'href=') hrefTrail=lis.find(r'target=\"_blank\"&gt;',hrefHeader) url[0]=lis[hrefHeader+6:hrefTrail-2] if hrefHeader!=-1 and hrefTrail!=-1: for times in range(1,20): hrefHeader=lis.find(r'href=',hrefTrail) hrefTrail=lis.find(r'target=\"_blank\"&gt;',hrefHeader) url[times]=lis[hrefHeader+6:hrefTrail-2] # print url[i] for i in range(0,20,2): articleHtml=requests.get(url[i],headers=header) articleHtml.encoding='gb2312' articleContent=articleHtml.text bsContent=BeautifulSoup(articleContent,'html.parser') title=bsContent.find('h2').string content=bsContent.find(class_=\"arwzks\") article=content.get_text() txt=article.encode('utf-8') print title+' start' # print txt open(r'/home/wukong/testTuiLiXue/download/'+title.encode(\"utf-8\")+'.txt','w+').write(txt) print title+' end' time.sleep(1)print 'finish'","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"-python -爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"三个步骤搞定一个爬虫--源码","slug":"三个步骤搞定一个爬虫-源码","date":"2017-11-19T07:18:57.000Z","updated":"2017-11-19T07:29:34.969Z","comments":true,"path":"2017/11/19/三个步骤搞定一个爬虫-源码/","link":"","permalink":"http://damiantuan.xyz/2017/11/19/三个步骤搞定一个爬虫-源码/","excerpt":"","text":"这里给出“三个步骤搞定一个爬虫的python源码” 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#-*-coding:utf8-*-#!usr/bin/pythonimport urllibimport timeurl=['']*10title=['']*10# 循环页数for page in range(1,39): pageContent=urllib.urlopen('http://tuilixue.com/zhentantuilizhishi/list_4_'+str(page)+'.html').read() # print pageContent #这一部分先找到某一页中的第一篇文章作为初始化 #根据网页代码特性，发现只有需要的页面文章链接前面有/span&gt;&lt;a href=，于是找到页面中的/span&gt;&lt;a href=的下标开始 hrefHeader=pageContent.find(r'/span&gt;&lt;a href=') # print hrefHeader #发现所需文章链接后面没有唯一的特性，于是从前面找到的下标开始找第一个target=\"_blank\"&gt;的下标 hrefTrail=pageContent.find(r'target=\"_blank\"&gt;',hrefHeader) # print hrefTrail #根据找到的下标，增加一定的位数找到文章链接，就是一个切片操作 url[0]=pageContent[hrefHeader+15:hrefTrail-2] #寻找文章标题 titleHead=pageContent.find(r'target=\"_blank\"&gt;',hrefHeader) titleTrail=pageContent.find(r'&lt;/a&gt;',titleHead) title[0]=pageContent[titleHead+16:titleTrail-1] if hrefHeader!=-1 and hrefTrail!=-1: for times in range(1,10): # 从前面的找到的尾部下标开始寻找 hrefHeader=pageContent.find(r'/span&gt;&lt;a href=',hrefTrail) #从上一步开始寻找 hrefTrail=pageContent.find(r'target=\"_blank\"&gt;',hrefHeader) #根据找到的下标，增加一定的位数找到文章链接，就是一个切片操作 url[times]=pageContent[hrefHeader+15:hrefTrail-2] #寻找文章标题 titleHead=pageContent.find(r'target=\"_blank\"&gt;',hrefHeader) titleTrail=pageContent.find(r'&lt;/a&gt;',titleHead) title[times]=pageContent[titleHead+16:titleTrail-1] for times in range(0,10): #分别读出每一个文章的url articleContent=urllib.urlopen(url[times]).read() print \"Start download~~\"+str(url[times][-9:]) #以html的格式保存 open(r'/home/wukong/testTuiLiXue/download/'+url[times][-9:].replace('/',''),'w+').write(articleContent) print \"Download finish\"+str(page-1)+str(times) #缓存时间为1s time.sleep(1)print \"All finish\"","categories":[],"tags":[]},{"title":"三个步骤搞定一个爬虫（3）","slug":"三个步骤搞定一个爬虫（3）","date":"2017-11-19T05:32:50.000Z","updated":"2017-11-19T05:52:22.614Z","comments":true,"path":"2017/11/19/三个步骤搞定一个爬虫（3）/","link":"","permalink":"http://damiantuan.xyz/2017/11/19/三个步骤搞定一个爬虫（3）/","excerpt":"上两篇文章讲了怎么把整页的文章下载下来，这一次讲一下怎么把整个专栏的文章都下载下来。讲到这里，一个基础的爬虫基本上都完成了，所以技术方面这些东西并不是有什么神秘的，只是很多东西没有学到，没有思路就会觉得不可思议而已。","text":"上两篇文章讲了怎么把整页的文章下载下来，这一次讲一下怎么把整个专栏的文章都下载下来。讲到这里，一个基础的爬虫基本上都完成了，所以技术方面这些东西并不是有什么神秘的，只是很多东西没有学到，没有思路就会觉得不可思议而已。 在我们开始之前我们先把上次遗留下来的问题解决一下。大家想一下，我们在之前是直接就去循环文章的链接了，尽管我们是根据html文档来进行分析判断的，但是似乎还是不够严谨，因为万一我们的判断失误了呢，那么程序就会没有响应并且一直卡在那里，因此我们应该加一个判断。 因为我们知道find函数在没有找到目标字符的时候返回-1,所以我们应该判断在不返回-1的情况下才对链接进行循环遍历。 开始工作分析源码 好了，这次我们是要下载整个推理讨论板块的所有文章，我们还是对网站进行分析先。 这是第一页的网址，我们再来看看第二页 再来看看第三页 很好，直觉告诉我们，最后的那个数字应该就是页数的意思了，我们试试改成1看看能不能回到第一页。 成功的回到了第一页，那么我们来看看需要循环几次 一共有38页，372篇文章，我们先来写一个循环看看能不能正确循环出页数 因为字符串是不能和整型相加的，并且urllib.openg也只能接受字符参数，所以我们这里用str（）把页数的数字转换为字符型，并且连接在网址上。测试一下 篇幅原因我就不全部截图了，但是我们可以看到，这个循环应该是可以把所有的页数都正确的表示出来的，那么我们现在就要把这个循环语句加到程序中去了。大家想想，我们应该是先到一个具体的页数，然后再爬取该页的链接和文章吧，所以我这个地方的循环应该是加在最外面的一个循环。 这样我们的爬虫基本就完工了，下面我们来做一些其他的事情。首先是对于网站的压力应对，我们现在写的是一个小爬虫，对网站不能造成什么压力，而且是单线爬虫，如果以后写多线程和爬取的内容多的时候，难免造成对网站的请求过于平凡，很多时候爬虫就会被服务器封杀掉，这时我们就要限制爬虫的速度了。我们先来导入一个模块 然后在每次下载完成的时候让程序暂停一下，我这里设置的是1秒钟 然后我们再来看看我们的爬虫，大家不知道有没有发现，我们下载的文章的保存名字似乎是一个递增的数字，那么我猜想可能是这个网站累计文章的篇数，为了证实这个猜想，我们到最后几页去看看。我们先来看看37页的html代码. 这是我们发现我们用来命名文件的数字由4位数变成了3位数，又变成了2位数，我们这个时候在来看看我们的下载保存路径。 这时我们要注意到，如果文章命名的数字变成2位数的时候，命名的切片操作会把文章链接中的/符号也加进去，这时就会构成一个新的下载路径了，由于我们并没有这样的路径，所有程序就会报错。具体错误是没有一个这样的文件指向。这个是怎么样的情形大家可以自己去实验一下。下面我们就要解决这个问题，想办法把/替换掉或者去掉。我们当然可以用find找出最后的/的下标，再从下标加1的地方去执行命名，但是这样无疑会多写出几行代码来，而且每次都要去判断，之后命名的代码也要重写，这样似乎工作量就上去了。我们可以换一个想法来解决这个问题，比如说在原有的命名代码中，只要发现有/字符存在的，一律替换为空或者其他。下面我们来看一个函数。 字符串的replace函数可以将字符串中的目标字符替换成其他的字符，这时我们来这样写。 就这样，我们就可以轻松的把我们切片中的/给去掉了我们运行一下看看是否可以下载。先看看目录下面有什么。 这些都是上一次我们下载的东西，我们删除掉。 现在目录已经是空的了，我们现在开始下载试试。 一堆end之后我们的文件下载好了，我们去看看. 一大堆的文件，我们看看数量 372个，刚好对应了372篇文章。就这样，我们就完成了一个定制的爬虫。","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"python,爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"三个步骤搞定一个爬虫（2）","slug":"三个步骤搞定一个爬虫（2）","date":"2017-11-19T02:35:23.000Z","updated":"2017-11-19T05:30:09.210Z","comments":true,"path":"2017/11/19/三个步骤搞定一个爬虫（2）/","link":"","permalink":"http://damiantuan.xyz/2017/11/19/三个步骤搞定一个爬虫（2）/","excerpt":"第一篇文章写了怎么去获取一篇文章，这一篇文章就写写怎么把一页中的10篇文章全部获取到。","text":"第一篇文章写了怎么去获取一篇文章，这一篇文章就写写怎么把一页中的10篇文章全部获取到。 开始工作获取其他链接 上一次我说了怎么去获取第一条的文章链接，现在我们再来爬取本页后面剩下的链接。我们先来看看上次我们爬取链接用的代码。 获取后面的链接我们能不能如法炮制呢，我们先来试试。我们把代码写成下面那样 然后我们现在来试试 结果我们发现我们试图获取的三条链接都是一样的，可以看出，这还是本页的第一篇文章的链接。证明我们这种方法是不可行的。我们回想一下上一节课我们讲的定位链接使用函数。 就是这个find函数，我们看看帮助，我们发现了我们可以自定义开始寻找的下标和寻找结束的下标。我们从html里面发现我们想要爬取的链接相隔都不是很远，都处在同一个div下面。于是我们来试试，从第一条链接后面开始寻找第二条链接。 这里我们要注意后面两条代码，我们选择了开始的下标是从上一条链接的尾部开始的。现在我们来试试是否可以获取正确的链接。 现在我们获取到了三条不同的链接，我们再通过对比html来看看我是否获取的是正确的链接。 从结果来看，我们的代码成功的获取了本页的前几篇文章的链接。关于怎么获取剩下的链接我们应该有头绪了。当然，这里一页只有10篇文章，也就是只有10个链接，我们可以把我们的获取链接的代码复制10次，可是如果一页有20篇，30篇，50甚至是100篇呢，难道我们也要将代码复制那么多的次数，肯定不能，也不科学。很多人现在已经知道要用循环来做了，但是这个要怎么循环，从哪里循环呢？我们再来看看我们上面的代码，我们发现除了第一条链接获取的代码不一样，后面两条链接获取的代码都是一样的，这时我们就知道我们应该从第二条链接获取代码进行循环了。 这里我们要先定义一个列表对获取的链接进行存储，因为是10篇文章，所以这里定义的就是一个10个元素的空的字符串列表。下面是我们循环的代码块。 这里结束一下我们为什么不是从0开始进行赋值，大家注意到没有，我们是从第二条文章链接看是循环的，那么第一条的文章链接在哪呢？当然是存储在了列表的第一个位置，也就是下标为0的那个位置了，关于range后面的范围，大家知道是包下不包上的就行了，就是说在range（x，y）的循环中，循环是从x开始，到y-1结束的，不包括y本身。我们现在来运行一下我们的代码看看是否获取的是正确的链接。 然后再次对比html 这时我们发现我们获取了链接是正确的，那么我们就要开始进行下载了。还是上一节课的代码，不过我们进行修改一些地方。因为上次只是单个链接，这次我们有一个链接列表，所以我们应该采取循环进行下载。我们要对下载重新写一个循环了。 ) 我们现在来试试，这是上一节课我们成功下载的第一篇文章。 我们现在删掉他。 现在我们看到文件夹里面是什么都没有的，我们现在开始下载。 我们这就下载完了，我打开其中一个看看。还是注意地址栏上面的链接。 可以看出我们获取的文章是正确的。因为篇幅有限，我就不一个一个去打开截图了，大家自己可以根据自己实际环境敲一下代码。","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"python,爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"三个步骤搞定一个爬虫（1）","slug":"三个步骤搞定一个爬虫（1）","date":"2017-11-18T14:57:45.000Z","updated":"2017-11-19T03:47:50.383Z","comments":true,"path":"2017/11/18/三个步骤搞定一个爬虫（1）/","link":"","permalink":"http://damiantuan.xyz/2017/11/18/三个步骤搞定一个爬虫（1）/","excerpt":"这篇文章算是我刚刚开始学爬虫的时候写的一个笔记了，现在整理出来，也算是一个分享过程吧。爬虫总体归纳来说就是三个步骤，获取网页源码，定位信息，提取保存信息，我们一个一个来说。","text":"这篇文章算是我刚刚开始学爬虫的时候写的一个笔记了，现在整理出来，也算是一个分享过程吧。爬虫总体归纳来说就是三个步骤，获取网页源码，定位信息，提取保存信息，我们一个一个来说。 什么是爬虫 我们都用过搜索引擎，嗯，就是Google，百度，必应之类的东西，我们为什么可以在上面搜索出东西来就是因为他们有爬虫程序在后台帮他们收集数据。就像我们要从网上获取数据我们就要去浏览网页，爬虫就是充当了一个浏览网页的机器人，将获取到的信息返回给自己的服务器。 目标 从网页上获取一篇文章 思路 其实我们不必对爬虫感到很神秘，毕竟我们每天都在用的东西。爬虫其实本质上就是在模拟用户的浏览行为，只要我们抓住这一点的话我们就可以围绕这点来展开了。用户浏览网页首先是要打开网页，再从网页上面获取到自己需要的信息，我们可以用爬虫来完成这一行为。 方法 尝试获取到网页的源码，再从中提取数据。 准备工作 因为本人一直对推理悬疑比较感兴趣，所以这次爬取的网站也是平时看一些悬疑故事的网站，同时也是因为这个网站在编码上面和一些大网站的博客不同，并不那么规范，所以对于初学者还是有一定的挑战性的。演示系统用的是kali，因为懒得去配置各种py模块了，就利用系统已经配置好的，浏览器是firefox，使用的IDE是微软的vscode。python版本为2.7 查看robots.txt 首先我们选取了我们要爬取的网站http://tuilixue.com/，先检查一下robots.txt看看是否存在有一些反爬虫的信息。 很好，这里没有什么限制 查看目标 然后我们到我平时比较常去的板块看看，http://tuilixue.com/zhentantuilizhishi/list_4_1.html我们现在想要爬取的文章就是这样的 右击鼠标查看源代码，我们可以看到，我们想要爬取的链接就是这样的 来一张清晰的 开始工作获取网页源码 但是我们要怎么办才能使python得到这个网页的源代码呢？我们可以使用python的urllib模块提供的open方法，首先我们先新建一个py文件，惯例12#-*-coding:utf8-*-#!usr/bin/python 因为是linux系统，所以python路径不同于windows，第一行代码说明是用的uft-8进行编码 在这里我们要先导入urllib这个模块，使用import导入。这里其实是两个方法，一个open一个read，open用于从网站上获取网页代码，read是为了读出来好打印。 我们可以得到上面结果，但是我们发现字符似乎成了乱码，为了找到原因，我们再来看看源码。 我们似乎找到了原因，网页使用的是gb2312进行编码的，但是我们是使用utf-8的，所以导致的乱码，对这方面不解的同学可以去找一些编码的知识看看。下面我们用一个编码转换来尝试获取正确的编码。 这时可以看到，我们通过强制的编码将获取的网页重新通过gb2312进行编码，我们就可以看到正确的字符了，但是在我们的这次课中并不需要这样的转码，这里只是为了显示获取的是正确的网页，从图中看到，我们获取的正是我们需要进行爬取的页面。 分析源码 下一步，我们需要获取我们本页的所有的文章链接了，这里需要有一点html和css的知识，关于这部分的知识，大家自己去掌握就行了，不需要太深入。如图中显示的，href后面的就是我们在本次课中需要爬取的链接，每页都有10篇文章是我们需要爬取的，我们先从第一篇的链接开始。 这时候我们就要想我们应该怎么样去获取到这个页面的链接了，如果正则表达式好的同学应该是想到了采取正则表达式进行获取，但是这里有一个问题，一个html页面中有如此多的a开头的元素，也有如此多的href开头的元素，想要通过正则去定位还是有点难的，就算定位出来，也是一大堆的代码，这就不利于可读性了。这时我们应该再从html文本中去分析。我们使用type函数进行类型的判断。 通过对pageContent的类型分析，我们知道这是一个字符串类型，这样我们就可以使用字符串中的find函数了，我们需要对find函数有一个了解。 函数中说明了从字符串中寻找目标字符，返回找到的第一个下标，如果没有找到就返回-1,同时可以设置开始寻找的位置和结束的位置。我们再看到文本。 我们发现是在div class=“liszw”下的li元素中的a元素中含有我们需要的链接，这时我们一个个来分析。 这时我们发现这和我们所要爬取的链接数量上是完全吻合的。我们就来试试。 这里我们采取了一个切片操作，这时我们发现链接其实已经爬取到了，但是还是有些不完美，我们再来完善一下他。 我们来对比一下我们的网页上的第一个链接 保存信息 这样我们就成功的爬取了第一个链接，现在我们来准备下载第一篇文章。从前面我们可以知道，我们可以把网页通过python的urllib模块下载下来，那么同样的道理，我一样也可以通过urllib模块对文章进行下载。我们通过链接的最后一串数字对下载下来的文件进行命名。并在下载玩后打印end进行提示。 我们可以看到，路径下是没有文件的。现在我们开始下载。 从这里看我们的文件应该是下载成功了，我们去文件路径下面看看。 文件下载是成功了，我们来打开看看。这个地方要注意地址栏的链接 这样，通过三个步骤，我们的爬虫就已经完成了。","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"python,爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"我的第一篇文章","slug":"我的第一篇文章","date":"2017-11-18T06:09:13.000Z","updated":"2017-11-18T07:54:11.772Z","comments":true,"path":"2017/11/18/我的第一篇文章/","link":"","permalink":"http://damiantuan.xyz/2017/11/18/我的第一篇文章/","excerpt":"其实第一篇文章也不知道说什么好，简单的就说说搞博客的这件事吧。","text":"其实第一篇文章也不知道说什么好，简单的就说说搞博客的这件事吧。 第一篇文章 其实第一篇文章也不知道说什么好，之前对前端的知识只是掌握了一点HTML+CSS，其他的不会太多，也没有用Markdown语法写过什么东西，这是第一次尝试，这第一篇文章就当作是一个test吧，因为我也不知道有什么效果。因为第一篇文章，我也想随便写点什么东西，简单的就说说搞博客的这件事吧。 来说说为什么开个博客 这个问题其实也很好解释，朋友圈已经沦陷了，成了微商和投票专用的通告栏，还成为了七大姑八大姨八卦的云聊天室，所以个人微信上已经不怎么用了。关于QQ空间这个其实和微信的朋友圈差不多，成了卖东西的地方和鸡汤遍地之所，而且朋友们很多都已经不玩QQ空间了，而且空间也并不是一个开放的地方，同时很多敏感的代码会被和谐掉，所以形同鸡肋。而且朋友中搞这个的人并不太多，交流还是少的。 关于hexo hexo的好处有很多，什么简单啊，可以免费使用github的空间之类的，这些文章网上有很多了，一搜就能够搜到一大堆，其中不乏大牛的源码分析，我也写不出什么东西来。只能说选这个静态博客框架纯粹是因为自己懒，不想去搞服务器也不想去配置什么东西了，而且别到时候没有续费就把东西全部清理掉了那才是坑，也不想到别的什么博客平台去注册什么账号的，因为很有可能自己写的文章就突然间不见了，就想着简简单单的写点东西，第一个算是自己在学习的路上做个笔记，第二个也是可以分享出来给大家看，技术这点东西其实没什么可以隐瞒的，不过你学得比人家多你就知道一些人家暂时还不知道的东西而已，多分享一下自己踩过的坑，后面的人的路就会顺一点。同时也是督促自己要去学习吧，毕竟博客总不更新会觉得怪怪的，说我好学也好，装哔也罢，不动脑不动笔写点东西，整个人都快生锈了。 关于博客 开个博客总要写点什么吧，其实我也不太清楚最后这个博客会变成什么样子，慢慢写的，自己学到什么东西就写点什么东西吧，不过总的来说应该都是python方面的学习笔记，偶尔会有点经济学，经济行为学和心理学与管理学方面的文章穿插进来。就这样吧，像背景音乐中的一句，就老去吧，孤独别醒来，你渴望的离开，只是无处停摆。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://damiantuan.xyz/categories/随笔/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://damiantuan.xyz/tags/随笔/"}]}]}