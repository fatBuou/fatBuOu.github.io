{"meta":{"title":"哟，一个大面团","subtitle":null,"description":"太阳那么红，为什么还那么冷","author":"PG-55","url":"http://damiantuan.xyz"},"pages":[{"title":"分类","date":"2017-11-16T07:07:10.000Z","updated":"2017-11-19T08:00:57.235Z","comments":true,"path":"categories/index.html","permalink":"http://damiantuan.xyz/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2017-11-15T06:54:21.000Z","updated":"2017-11-15T07:39:31.140Z","comments":true,"path":"tags/index.html","permalink":"http://damiantuan.xyz/tags/index.html","excerpt":"","text":""},{"title":"说个笑话","date":"2017-11-15T06:04:44.000Z","updated":"2017-11-18T06:35:32.514Z","comments":true,"path":"about/index.html","permalink":"http://damiantuan.xyz/about/index.html","excerpt":"","text":"关于自己人究竟是理性的还是非理性的？经济学说人应该都是理性的人，而经济行为学却告诉我们人出于理性的行为是多么荒诞不经。管理学说做管理应该是理性的，而世界上的管理却有很多用屁股代替脑袋的事情。运筹学说如果一个整体的所有因素都是最佳的，那么这个整体将呈现最佳状态，而博弈论告诉我们囚徒困境。 “和其他人不一样的话，我会被笑话的”“所以我们都活成了笑话” 于是我带上面具为人们带来了笑话，结果到最后发现自己就是个笑话。"}],"posts":[{"title":"爬取薄荷网所有食物的营养数据，并分类放入excel中","slug":"爬取薄荷网所有食物的营养数据，并分类放入excel中","date":"2020-03-30T10:30:39.000Z","updated":"2020-03-31T11:13:57.847Z","comments":true,"path":"2020/03/30/爬取薄荷网所有食物的营养数据，并分类放入excel中/","link":"","permalink":"http://damiantuan.xyz/2020/03/30/爬取薄荷网所有食物的营养数据，并分类放入excel中/","excerpt":"","text":"最近想要减肥，想着先从食物入手，在网上查了一下，发现有一个叫薄荷的网站还是不错的，食物都做了详细的分类，并且所有的营养都做了数据的量化，作为制定饮食计划的参考还是不错的。打算先大致的看一下常见的食物营养做到心中有数，再来制定计划。我同时还发现了网上有一篇文章，已经有人爬过薄荷了，但是只是在列表也爬取了热量数据，并没有进到详情页去爬取其他的营养参数。 我们先进到薄荷的食物页面，发现网站将所收录的食物分成了11个大类，我想是将每一个大类放进一个excel表格的sheet里，那么这样的话就有11个sheet了。 再来看到每一个分类的链接，发现用了短链，在短链的前面应该加上网站的域名’www.boohee.com’，那么先把分类页面爬取下来即可。 导包侠，先预测大概需要那些包123456789from multiprocessing import Processfrom multiprocessing import Poolimport osimport requestsfrom bs4 import BeautifulSoupfrom lxml import etreeimport pandas as pdimport openpyxlimport time 1234567891011def group_url(): url = 'http://www.boohee.com/food/' res = requests.get(url) html = res.text bs = BeautifulSoup(html,\"html.parser\") #装每种食物种类的url li = bs.find(class_='row').find_all('li') #每种食物大类的url for f in li: #f代表每种食物大类 group_url = f.find(class_=\"img-box\").find('a')['href'] group_url = 'http://www.boohee.com'+group_url yield group_url #group_url是每一种食物大类的url 因为设计是爬取了一个大类就写一次sheet，所以这里使用了yield，每爬取一次处理一次，剩下的后续等待。 进到分类里面之后就是该分类下面的食物列表了，我发现每一类下面食物大概都有10页的，一页有10种食物，那先把所有的翻页链接拿下。 1234567def book_url(group_url): urls = [] #存储每类食物中下的翻页 for i in range(1,11): page_url = group_url+'?page='+str(i) urls.append(page_url) # urls存储的是每一类食物中的所有页 return urls 在具体的一页中获取该页的所有食物链接，并进入每个链接中获取食物的所有信息。 123456789101112131415161718def get_info(urls): data = [] food_end = [] pool = Pool(processes=4) info = [] info_url = [] #食物详情的url s = pool.map(requests.get,urls) for i in s: html = i.text bs = BeautifulSoup(html,\"html.parser\") group = bs.find(class_='widget-food-list pull-right').find('h3').text #食物类别 end = bs.find_all(class_='img-box pull-left') for j in end: info_url.append('http://www.boohee.com&#123;0&#125;'.format(j.find('a')['href'])) data.append(pool.map(get_detil,info_url)) frame = pd.DataFrame(data=data[0] ,columns=['名称' ,'又名' ,'热量（大卡）' ,'碳水化合物（g）' ,'脂肪（g）' ,'蛋白质（g）' ,'建议']) return group ,frame 上一段代码中的get_detil就是具体在详情页面中定位各种营养数据的实现过程 1234567891011121314151617181920def get_detil(url): # 获取详情页面的具体信息 print(url) s = requests.get(url = url) if s.status_code != 200: N = ('', '', '', '', '', '', '') return N text = s.text html = etree.HTML(text) try: name = html.xpath('/html/body/div[1]/div[3]/div/h2/a[2]')[0].tail #名字 except Exception as e: name = '' other_name = html.xpath('/html/body/div[1]/div[3]/div/div[2]/div[1]/div/ul/li[1]/b')[0].tail #别名 calories = html.xpath('/html/body/div[1]/div[3]/div/div[2]/div[2]/div/dl[2]/dd[1]/span[2]/span')[0].text #热量 carbohydrate = html.xpath('/html/body/div[1]/div[3]/div/div[2]/div[2]/div/dl[2]/dd[2]/span[2]')[0].text #碳水化合物 adipose = html.xpath('/html/body/div[1]/div[3]/div/div[2]/div[2]/div/dl[3]/dd[1]/span[2]')[0].text #脂肪 protein = html.xpath('/html/body/div[1]/div[3]/div/div[2]/div[2]/div/dl[3]/dd[2]/span[2]')[0].text #蛋白质 suggestion = html.xpath('/html/body/div[1]/div[3]/div/div[2]/div[1]/p/b')[0].tail #建议 return name ,other_name ,calories ,carbohydrate ,adipose ,protein ,suggestion 上面的get_detil的对名字做处理是后面运行的时候才加的，主要是发现了有一些食物在列表页面是有名字的，但是到了详情页面中的时候，名字字段就缺失了，这种情况下xpath的定位就会发生异常，所以我们这里就这样处理先。一开始想在列表也上用名字的，但是这样将会耗费大量的时间在请求上，所以用详情页中的面包屑导航中的信息，但有些面包屑会缺失掉一点。 这是正常的面包屑 这是异常的面包屑，缺失了分类信息 爬取到的信息要保存，因为信息不是很多，而且我这台电脑上没有安装数据库，所以我就用excel进行保存了。不同的分类保存在不同的sheet里，一开始想直接用pandas模块的excel写入方法的，后面发现不行，找了挺多资料，也做了很多的尝试，发现用openpyxl模块可以实现。 12345678def save_excel(data,sheet_name): writer = pd.ExcelWriter('Excel_test.xlsx',engin='openpyxl') book = openpyxl.load_workbook(writer.path) writer.book = book df = data df.to_excel(excel_writer=writer,sheet_name=sheet_name) writer.save() writer.close() 后面就是把具体的实现步骤连接起来了。 123456789if __name__ == '__main__': start_time = time.time() url = group_url() for i in url: r = book_url(group_url=i) e = get_info(urls=r) save_excel(data=e[1],sheet_name=e[0]) end_time = time.time() print(end_time - start_time) 整个爬虫下来其实没有什么值得说的，如果是单线程的话耗时真的是太久，分析一下，这个爬虫其实最耗时应该是在获取详情的地方，写入excel的数据量并不是太大，所以针对请求做了多线程处理。一开始只是加了一处，发现处理起来快很多，后面在将爬取数据处理成dataFrame中也加入了多线程，整个爬虫的速度确实有变快。 这个爬虫不是太难，就简单的做一个练手的记录吧。","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"python,爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"爬取深圳证券交易所专题统计","slug":"爬取深圳证券交易所专题统计","date":"2018-06-08T21:30:39.000Z","updated":"2018-06-09T10:30:22.026Z","comments":true,"path":"2018/06/09/爬取深圳证券交易所专题统计/","link":"","permalink":"http://damiantuan.xyz/2018/06/09/爬取深圳证券交易所专题统计/","excerpt":"","text":"有一段时间没有更新博客了，其中原因很多，先是接下了一个公司的工作，当时项目很急，也经常加班，也造成了没有时间去更新博客，后面因为工作中的一些理念不同，我也离开了那个公司到别的地方去工作了，换了新工作很多东西需要去学习和熟悉，换了工作也换了住的地方，这也花费了很多的时间去处理。本来这篇文章在过年的时候就想写的了，结果一晃都半年过去了，现在才动笔。 首先还是老规矩，做爬虫，先分析网络请求。其中的目标URL是深圳交易所专题统计，我们要爬取的就是其中的主要财务指标，如图所示. 这样看到这些报告的链接，很多都直接是指向文件的链接，这意味着我们如果直接访问这些链接就相当于是直接访问文件了，如果要保存到本地的话我们只需要用二进制讲访问返回的内容写到本地即可。多翻几页能看出来，文件都是doc，docx，pdf，xlsx，还有shtml之类的组成的，这些不影响，还是采取使用二进制写进到本地进行存储，得到文件之后再进行处理。 我们再往下看，发现一共有10页，我们尝试点击翻页，看看浏览器上面的链接有没有上面变化。结果我们发现，并没有在url中暴露翻页的信息。 如此这般，那么只只好抓包分析了，上F12，然后点解下一页分析一下请求的情况。 这样看来就很明显了，请求不是很多，不得不说金融行业真是话不多，但是说话都命中要害，减少了很多不必要的开销。看图我们发现是一个post请求，让后返回了一个HTML页面，我们可以看到返回的HTML中有第二页所出现的报告的名字和相应的链接，那么我们也不用去分析别的链接了，就是这个链接了，下面进行分析。 我们可以看到，翻页的时候触发了一个js，再加上一个随机的浮点型数字，我当时翻了一下网页中的代码，奈何金融行业比较社会，人狠话不多，基本没看懂这个随机数是怎么来的，并且每次点击翻页的随机数都不一样，从这个地方看，我基本就放弃使用requests或是urllib来进行访问了。不过爬虫与反爬就是这样子，你有张良计，我有过墙梯，直接使用python发包不行的话就只能借助别的工具来进行访问了。 介绍一个小工具，phantomjs，网上有很多这个工具的教程，重复phantomjs入门使用就不在这里赘述了，关于这个工具，大家可以理解成一个没有界面的浏览器即可。这个小工具下载好了直接双击，然后就会自动在系统里面了，使用的时候只需要将phantomJS和脚本放在一起就行了。 首先，我们先来实例化一个phantomJS无头浏览器。1driver = webdriver.PhantomJS() 因为我们第一次打开目标URL的时候其实时用的get请求，于是我们也可以尝试使用phantomJS来进行get请求。12driver.get(\"http://www.szse.cn/main/marketdata/zttj_front/\") #coding='ISO-8859-1'html = driver.page_source 我们发现，在这段代码中可以获取正确的HTML，于是我们再尝试定位到下一页的按钮，尝试是否可以正常点击下一页。1driver.find_element_by_xpath('//*[@id=\"Table1\"]/tbody/tr/td[3]/input[2]').click() 运行之后发现，在phantomJS的click()执行之后，确实是可以获取第二页的HTML，那么翻页暂时来说不是问题了，下面的问题就是怎么把每一页的链接获取到，然后下载文件到本地。我们在网页上点击文件的链接，发现可以直接打开一些文件，这时我们就可以在获取文件链接之后使用python进行请求发包了。来看看下面这段代码。123456789101112131415161718192021222324# 获取文件链接，此处doc包括但不限于doc文件，还报考页面中所有的链接，# 当然，更正确的叫法应该get_url，但是当时只想到了链接直接指向文档，就get_文档了def get_doc(html): source = html bsContent = BeautifulSoup(source, 'html.parser') urlContent = bsContent.find_all(class_='tdline2') for i in urlContent: a = i.find(\"a\") url = a.get('href') download_url = domain_url + url download(download_url)# 下载文件def download(download_url): # 记录下载过的文档的链接 with open(file='url_log.txt',mode='r+') as fp: log=fp.read() if download_url not in log: r = requests.get(url=download_url) with open(file=download_url[download_url.rfind('/') + 1:], mode=\"wb\") as code: code.write(r.content) fp.write(download_url + \"\\n\") else: pass 这样的话文件是下载下来的，但是也仅仅是下载下来了，里面的信息没有办法提取出来，关于这几种文件在python中怎么解析，相关的模块的文档也有，这里也不去介绍入门了。这里有一点需要注意的是，因为doc的兼容性问题，我们可以将其转换成docx再来进行读取，这个地方需要电脑安装了MS office或是wps，因为是调用这两个应用程序的组件进行转换的，此处需要声明一下。多的不说的，代码的注释还是比较详细的，我直接讲代码放出。 整个脚本的代码如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201# -*-coding:'utf-8' -*-import requestsfrom bs4 import BeautifulSoupimport osimport win32comfrom win32com.client import Dispatchfrom win32com import clientfrom docx import Documentimport xlrdfrom selenium import webdriverimport timefrom pdfminer.pdfparser import PDFParser,PDFDocumentfrom pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreterfrom pdfminer.converter import PDFPageAggregatorfrom pdfminer.layout import LTTextBoxHorizontal,LAParamsfrom pdfminer.pdfinterp import PDFTextExtractionNotAlloweddomain_url = 'http://www.szse.cn'# 分析网页，下载文件def parse_html(): driver = webdriver.PhantomJS() driver.get(\"http://www.szse.cn/main/marketdata/zttj_front/\") # coding='ISO-8859-1' html = driver.page_source for i in range(0,10): if i ==0: get_doc(html=html) else: try: driver.find_element_by_xpath('//*[@id=\"Table1\"]/tbody/tr/td[3]/input[2]').click() time.sleep(3) get_doc(driver.page_source) except Exception as e: break# 获取文件链接，此处doc包括但不限于doc文件，还报考页面中所有的链接，# 当然，更正确的叫法应该get_url，但是当时只想到了链接直接指向文档，就get_文档了def get_doc(html): source = html bsContent = BeautifulSoup(source, 'html.parser') urlContent = bsContent.find_all(class_='tdline2') for i in urlContent: a = i.find(\"a\") url = a.get('href') download_url = domain_url + url download(download_url)# 下载文件def download(download_url): with open(file='url_log.txt',mode='r+') as fp: log=fp.read() if download_url not in log: r = requests.get(url=download_url) with open(file=download_url[download_url.rfind('/') + 1:], mode=\"wb\") as code: code.write(r.content) fp.write(download_url + \"\\n\") else: passdef translate_doc(f): \"\"\" 将doc文件转换为docx \"\"\" word = client.Dispatch('kwps.Application') #如果电脑安装是MS office，应该使用word.Application word.Visible = 0 doc = word.Documents.Open(f) # 目标路径下的文件 doc.SaveAs2(f[0:-3]+'docx',12) # 转化后路径下的文件 doc.Close() word.Quit() parse_docx(f[0:-3]+'docx')def parse_docx(f): \"\"\" 读取docx中的表格 \"\"\" d = Document(f) t = d.tables[0] rows=len(t._cells) try: for row in range (1,(rows//t._column_count)+1): stock_number = t.cell(row,0).text stock_name = t.cell(row,1).text profits = t.cell(row,2).text per_earn = t.cell(row,3).text per_profits=t.cell(row,4).text per_cash=t.cell(row,5).text if t.cell(row,6).text=='': r_plant ='无预案' else: r_plant=t.cell(row,6).text print(stock_number, stock_name, profits, per_earn, per_profits, per_cash, r_plant) except Exception as e: print(e)def parse_docx2(f): ''' 此函数作用是读取纯文本docx文件 :param f:文件参数 :return: ''' document = Document(f) for paragraph in document.paragraphs: print(paragraph.text)def parse_pdf(f): fp = open(f, 'rb') # 以二进制读模式打开 #用文件对象来创建一个pdf文档分析器 praser = PDFParser(fp) # 创建一个PDF文档 doc = PDFDocument() # 连接分析器 与文档对象 praser.set_document(doc) doc.set_parser(praser) # 提供初始化密码 # 如果没有密码 就创建一个空的字符串 doc.initialize() # 检测文档是否提供txt转换，不提供就忽略 if not doc.is_extractable: raise PDFTextExtractionNotAllowed else: # 创建PDf 资源管理器 来管理共享资源 rsrcmgr = PDFResourceManager() # 创建一个PDF设备对象 laparams = LAParams() device = PDFPageAggregator(rsrcmgr, laparams=laparams) # 创建一个PDF解释器对象 interpreter = PDFPageInterpreter(rsrcmgr, device) # 循环遍历列表，每次处理一个page的内容 for page in doc.get_pages(): # doc.get_pages() 获取page列表 interpreter.process_page(page) # 接受该页面的LTPage对象 layout = device.get_result() # 这里layout是一个LTPage对象 里面存放着 这个page解析出的各种对象 一般包括LTTextBox, LTFigure, LTImage, LTTextBoxHorizontal 等等 想要获取文本就获得对象的text属性， for x in layout: if (isinstance(x, LTTextBoxHorizontal)): with open(str(f)[:-3]+'txt', 'wb') as f: results = x.get_text() print(results) f.write(results + '\\n')def parse_excel(f): workbook=xlrd.open_workbook(f) for sheet_name in workbook.sheet_names(): sheet=workbook.sheet_by_name(sheet_name) for i in range(1, sheet.nrows): name = sheet.row(i)[1].value deal_money = str(sheet.row(i)[2].value) print(name, deal_money)def parse_shtml(f): with open(f, 'r') as fp: shtml = fp.read() p = BeautifulSoup(shtml, 'html.parser') biaozhi = p.find_all(class_='MsoNormal') for i in biaozhi: print(i.get_text(), )def run(): # 遍历文件 PATH = r'D:\\huarun\\BankSpider_self\\szse_spider' # windows文件路径 files = os.listdir(PATH) for doc in files: if os.path.splitext(doc)[1] == '.docx': try: parse_docx(PATH + '\\\\' + doc) except Exception as e: parse_docx2(PATH + '\\\\' + doc) elif os.path.splitext(doc)[1] == '.doc' or os.path.splitext(doc)[1]=='.DOC': try: translate_doc(PATH + '\\\\' + doc) except Exception as e: print(e) elif os.path.splitext(doc)[1]=='.pdf': try: parse_pdf(PATH+'\\\\'+doc) except Exception as e: print(e) elif os.path.splitext(doc)[1]=='.xls': try: parse_excel(PATH+'\\\\'+doc) except Exception as e: print(e) elif os.path.splitext(doc)[1]=='.shtml': try: parse_shtml(PATH+'\\\\'+doc) except Exception as e: print(e)if __name__ == \"__main__\": parse_html() # run() 最后实现的效果是可以进行增量爬取，因为已经爬取过的都会被记录下来，然后在获取到相同链接的时候忽略掉该链接的下载和解析，并且将文件都解析成字符串，如何分析，此处便是后话了。下图是当时爬取的一些记录。","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"python,爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"用python检测窗体状态","slug":"用python检测窗体状态","date":"2018-04-05T03:01:46.000Z","updated":"2018-04-05T03:57:42.907Z","comments":true,"path":"2018/04/05/用python检测窗体状态/","link":"","permalink":"http://damiantuan.xyz/2018/04/05/用python检测窗体状态/","excerpt":"","text":"之前的文章说了在做客户端的爬虫的时候我们需要对爬取的应用程序进行进程的监控，对出现的异常进行处理。后面又发现，其实在Windows上，窗体和进程的似乎没有什么关系（不知道对不对，比较对Windows的图形化不是很熟悉，错了请指正），就是进程一直都是很正常的，但是窗体可能出现未响应的情况。这个问题我想大家其实都遇过，电脑卡的时候，点击一个窗口，然后窗口卡了一下，再点击的时候整个窗口就变白了，然后在左上角跳出一个窗体未响应的提醒。就是这个样子的。！image 这个其实涉及的知识点蛮多的，首先是win32方面的只是，然后还有mfc方面的知识，最后其实才是python方面的处理。我在网上找到了一个相应的文章判断窗体是否无响应，但是可能是我的水平不够高吧，对于一些解决方法看得云里雾里的。在着手解决的时候，自己old school的思维风格又占据了上风，强行要使用Windows系统进行窗体状态的监测，毕竟mfc这东西就是你Windows提出来的，不依靠你自身解决依靠谁。 在Windows方面，python给出的资料确实有点少得可怜，我也是到处收集到一点代码片段，再慢慢组合起来，慢慢的摸索的，连猜带蒙写出来的，其实我给出的代码我自己本身也还是有挺多的不清错的地方，就希望有这方面的大神可以给点参考。 还是写成了模块，对于工作上的信息进行了隐匿。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788# coding:utf-8import win32com.clientimport win32apiimport win32guiimport win32processimport win32pdhimport ctypesimport time\"\"\"该模块为了解决客户端在爬取数据的时候的窗体未响应问题\"\"\"class Listen_gui_status(): @staticmethod def check_gui(): # junk是CPU的一些运行字段，instances是所有的进程名称，类型为list junk, instances = win32pdh.EnumObjectItems(None, None,'process', win32pdh.PERF_DETAIL_WIZARD) # print(instances) proc_ids=[] proc_dict=&#123;&#125; ubank_proc=None for instance in instances: if instance in proc_dict: proc_dict[instance] = proc_dict[instance] + 1 else: proc_dict[instance]=0 # print(proc_dict) # print(proc_dict.items()) for instance, max_instances in proc_dict.items(): # 此段方法是统计同名进程有多少个 for inum in range(max_instances+1): # PdhOpenQuery：获取性能监视器数据查询句柄，返回一个句柄 hq = win32pdh.OpenQuery() ''' MakeCounterPath:使用PDH_COUNTER_PATH_ELEMENTS结构中指定的成员创建一个完整的计数器路径。 如果函数成功，则返回ERROR_SUCCESS。 如果函数失败，返回值是系统错误代码或PDH错误代码 ''' path = win32pdh.MakeCounterPath( (None,'process',instance, None, inum,'ID Process') ) print('inum',inum,'hq',hq,'path',path) # PdhAddCounter:添加计数器 counter_handle=win32pdh.AddCounter(hq, path) # CollectQueryData:收集指定查询中所有计数器的当前原始数据值，并更新每个计数器的状态码。 win32pdh.CollectQueryData(hq) # GetFormattedCounterValue：获取指定计数器数值 types, val = win32pdh.GetFormattedCounterValue(counter_handle, win32pdh.PDH_FMT_LONG) proc_ids.append((instance,str(val))) win32pdh.CloseQuery(hq) proc_ids.sort() for proc_name in proc_ids: if proc_name[0]==u'QQ.exe': ubank_proc=proc_name[1] # print(proc_name[1]) hWndList = [] qq_hWnd=[] # 获取窗口句柄 win32gui.EnumWindows(lambda hWnd, param: param.append(hWnd), hWndList) # print ('------------------------------') # print (hWndList) # print ('------------------------------') # 迭代 for hWnd in hWndList: # 获取窗口句柄对应的线程ID和进程ID threadId, process_id = win32process.GetWindowThreadProcessId(hWnd) if process_id==int(ubank_proc): # print(threadId, process_id,hWnd) ubank_hWnd.append(hWnd) for hWnd in qq_hWnd: ''' 关于IsHungAppWindow(),Microsoft官方给出的解释是： The return value is TRUE if the window stops responding; otherwise, it is FALSE. Ghost windows always return TRUE. 具体可以查看https://msdn.microsoft.com/en-us/library/windows/desktop/ms633526(v=vs.85).aspx ''' status = ctypes.windll.user32.IsHungAppWindow(hWnd) if status: time.sleep(30) else: passif __name__=='__main__': Listen_gui_status.check_gui()","categories":[{"name":"python win32","slug":"python-win32","permalink":"http://damiantuan.xyz/categories/python-win32/"}],"tags":[{"name":"python 爬虫 mfc","slug":"python-爬虫-mfc","permalink":"http://damiantuan.xyz/tags/python-爬虫-mfc/"}]},{"title":"用python检测进程状态","slug":"用python检测进程状态","date":"2018-04-05T01:11:52.000Z","updated":"2018-04-05T03:00:30.793Z","comments":true,"path":"2018/04/05/用python检测进程状态/","link":"","permalink":"http://damiantuan.xyz/2018/04/05/用python检测进程状态/","excerpt":"","text":"我们很多时候做客户端爬虫的时候，或者是在Windows上做项目的时候，我们都需要对目标进程进行监控，在目标进程出现异常的时候对我们的爬虫进行相应的异常处理。 当时做这个的时候思路比较局限，可能是因为自己old school风格的原因吧，就想着怎么去做Windows编程，去请求系统的API，后面发现如果是去请求系统的API的话，第一是过程比较复杂，第二是在python方面，对Windows的一些介绍也比较少，像win32，win32api之类的，很多资料都不齐全，因为这些资料很多时候是面对有Windows编程有需要，或是对Windows编程比较有经验的人用的，入门的比较少。 后面搞的时候还是请教了一些大佬，其中一个搞安全的大佬给了一个思路，你试试用安全的角度去考虑这个功能的实现。也要感谢自己当时在安全的路上跌跌撞撞的走过几年吧，这样一样就有了一个解决的思路，虽然比较猥琐，做了一下搬砖的工作，还是比较快解决的。用到了一个名叫psutil的模块。更多的详情，可以参考psutil的github主页psutil，功能还是很多很强大的。 惯例，直接上代码，代码注释非常的清晰。因为这个是工作中用到的一个模块，我用类进行了封装。对于进程名，因为工作问题我隐去了，换成了一个QQ的进程。 123456789101112131415161718192021222324252627282930313233# -*-coding:utf-8-*-# _by:lamimport psutilimport timeimport sysclass Listen_from_status(): \"\"\" 该模块为了解决客户端在爬取数据的时候的进程异常的问题 \"\"\" @staticmethod def listen(): # name为需要监测进程名 name = 'QQ.exe' # 获取系统此时所有的进程名字，进程pid，进程开始时间 for proc in psutil.process_iter(): if name in proc.name(): # psutil.Process()这个函数接收一个进程pid作为参数，获取进程的信息，开始时间，进程名等 p = psutil.Process(proc.pid) # 判断进程的状态 if p.status() == psutil.STATUS_RUNNING: pass else: time.sleep(30) returnif __name__=='__main__': Listen_from_status.listen() psutil中定义的进程的状态有这几种，当然，我们也可以用字符串去代替他的状态情况。状态的字符串就是STATUS_后面的名字换成小写字母即可。（）中提示的是适用的系统。 psutil.STATUS_RUNNING psutil.STATUS_SLEEPING psutil.STATUS_DISK_SLEEP psutil.STATUS_STOPPED psutil.STATUS_TRACING_STOP psutil.STATUS_ZOMBIE psutil.STATUS_DEAD psutil.STATUS_WAKE_KILL psutil.STATUS_WAKING psutil.STATUS_IDLE(OSX, FreeBSD) psutil.STATUS_LOCKED(FreeBSD) psutil.STATUS_WAITING(FreeBSD) psutil.STATUS_SUSPENDED(NetBSD","categories":[{"name":"python 爬虫 Windows","slug":"python-爬虫-Windows","permalink":"http://damiantuan.xyz/categories/python-爬虫-Windows/"}],"tags":[{"name":"python,爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"python模拟登录某网教师教育网","slug":"Python模拟登陆某网教师教育网","date":"2018-04-03T08:11:52.000Z","updated":"2018-04-05T04:02:28.000Z","comments":true,"path":"2018/04/03/Python模拟登陆某网教师教育网/","link":"","permalink":"http://damiantuan.xyz/2018/04/03/Python模拟登陆某网教师教育网/","excerpt":"","text":"本文转载自看雪论坛【作者】rdsnow 不得不说，最近的 Python 蛮火的，我也稍稍了解了下，并试着用 Python 爬取网站上的数据 不过有些数据是要登陆后才能获取的，我们每年都要到某教师教育网学习一些公需科目，就拿这个网站试试，关键是对网站的分析 打开浏览器，输入网站网址 http://www.jste.net.cn ，按F12调出浏览器的开发者工具，选中 Network ，并勾选 Preserve log，防止切换网页时信息丢失 网页上输入账号，密码输入“123456”，验证码输入“abcde”，验证码不要输正确的，否则密码错5次，会被网站锁定账号30个小时，验证码倒是可以随便错 登陆后（当然登陆不上，会跳转到另一个登陆页面），在开发者工具中看到与服务器的数据交换 第一个是get验证码图片的，第二个就是向网站提交数据的，点一下第二个信息 这是个 Post 请求，重点看红框中的提交数据，randomCode就是输入的验证码了，x,y应该是点击的按钮控件的位置了，有cookie后就没有提交这个数据了，可以忽视，returnURL、appId，encrypt每次都是一样的，也不用管他，重点是 reqId 和 req 这两个 key 的值了，reqId猜想是点击按钮时取到的时间戳，可以复制这个数据到验证下 Unix时间戳(Unix timestamp)转换工具 单位选毫秒，确实是刚刚提交数据的时间，就剩下一个数据了，这个key的数值很长，下面来寻找这个数据是从哪里的来的 可以看到 login.jsp 下可以看到 encode.js、string.js、des.js 从名字上就能看出这几个是用来加密提交数据的，右键 login.jsp，选择 “Open in Sources panel” 可以跳转到 “源” 选项卡，看到 ’login.jsp‘ 的源码，如果格式混乱，比如所有代码在一行中，不便于观看，可以点击界面下方 的中括号，开发者工具会自动给你重新格式化代码。 仔细分析 login.jsp 的代码，看到 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950function doOk(frm) &#123; var el = frm.elements[\"loginName\"]; var loginName = el.value.replace(/ /g, \"\"); el.value = loginName; if (isEmpty(loginName)) &#123; alert(\"请输入登录名\"); el.focus(); return false; &#125; el = frm.elements[\"pwd\"]; el.value = el.value.replace(/ /g, \"\"); var pwd=el.value; if (isEmpty(el.value)) &#123; alert(\"请输入登录密码\"); el.focus(); return false; &#125; var d = new Date(); pwd = encode(loginName, pwd);//密码第一次加密，可以跟进 frm.elements[\"encrypt\"].value = \"1\"; var validCode=\"\"; el=frm.elements[\"randomCode\"]; if(el)&#123; el.value=el.value.replace(/ /g,\"\"); if (isEmpty(el.value)) &#123; alert(\"请输入登录密码\"); el.focus(); return false; &#125; validCode=el.value; &#125; loginName=encodeURI(loginName);//避免中文问题 进行URL编码 var reqId=(new Date()).getTime()+\"\";//获取时间戳给 reqId var str=strEnc(loginName+\"\\n\"+pwd,reqId,validCode);//关键加密代码，可以跟进分析 frm.elements[\"loginName\"].disabled=\"true\"; frm.elements[\"pwd\"].value=pwd; frm.elements[\"pwd\"].disabled=\"true\"; frm.elements[\"req\"].value=str; frm.elements[\"reqId\"].value=reqId; return true; &#125; 找到这段代码，其中主要是对输入检查的部分，重点看这两处 1pwd = encode(loginName, pwd); 此处对密码进行第一次加密 1234loginName=encodeURI(loginName);//避免中文问题var reqId=(new Date()).getTime()+\"\";var str=strEnc(loginName+\"\\n\"+pwd,reqId,validCode); 第一行：将用户名进行 URL 的格式编码 第二行，取时间戳赋值给 reqId 第三行传入用户名，加密后的密码和验证码进行验证，函数返回值赋给变量 str，正是提交数据的 req 的值 在两个加密函数入口设置断点，开发者工具设置断点的，只要在这个代码的行号上点击鼠标就行了，设好断点后，再次输入用户名密码和验证码，重新提交，程序被断下： F11单步进入第一个断点，这里需要点击界面下面的中括号重新格式化下代码，单步跟进后看到： 12345678910111213141516171819202122232425262728293031323334353637var _$_7151 = [\"encode\", \"ABCDEFGHIJKLMNOP\", \"QRSTUVWXYZabcdef\", \"ghijklmnopqrstuv\", \"wxyz0123456789+/\", \"=\", \"\", \"charCodeAt\", \"charAt\", \"length\", \"join\", \"reverse\", \"split\"];window[_$_7151[0]] = function(c, e) &#123; function a(p) &#123; var q = _$_7151[1] + _$_7151[2] + _$_7151[3] + _$_7151[4] + _$_7151[5]; p = encodeURI(p); var r = _$_7151[6]; var g, h, j = _$_7151[6]; var k, l, m, o = _$_7151[6]; var b = 0; do &#123; g = p[_$_7151[7]](b++);//第一个字符 h = p[_$_7151[7]](b++);//第二个字符 j = p[_$_7151[7]](b++);//第三个字符 k = g &gt;&gt; 2; //得到 k l = ((g &amp; 3) &lt;&lt; 4) | (h &gt;&gt; 4);//得到 i m = ((h &amp; 15) &lt;&lt; 2) | (j &gt;&gt; 6);//得到 m o = j &amp; 63; //得到 o if (isNaN(h)) &#123; //如果没有第二个字符 m = o = 64 //则取表中的第64个字符替换 &#125; else &#123; if (isNaN(j)) &#123; //如果没有第三个字符 o = 64 //则取表中的第64个字符替换 &#125; &#125; ;r = r + q[_$_7151[8]](k) + q[_$_7151[8]](l) + q[_$_7151[8]](m) + q[_$_7151[8]](o); g = h = j = _$_7151[6]; k = l = m = o = _$_7151[6] &#125; while (b &lt; p[_$_7151[9]]);;return r &#125; var d = c[_$_7151[9]]; var f = a(e)[_$_7151[12]](_$_7151[6])[_$_7151[11]]()[_$_7151[10]](_$_7151[6]); for (var b = 0; b &lt; (d % 2 == 0 ? 1 : 2); b++) &#123; f = a(f)[_$_7151[12]](_$_7151[6])[_$_7151[11]]()[_$_7151[10]](_$_7151[6]) &#125; ;return f&#125; 这个函数返回的 f 就是密码第一次加密后的结果了，这个代码是用什么工具变成这样的不太清楚，如果出现 _$_7151[n] 这样的字符可以查询代码最上面的列表 代换，大致过程不详说，跟一遍就知道了，就是循环从密码中取三个字符 g、h、j，然后将三个字符的ascii码左移或右移，或和其他结果加加减减，得到的结果 k、l、m、o 查询表格替换字符，如果密码长度不是 3 的整数倍，则查表结果用 “=” 替换，将循环得到的查表结果依次连接，并反序，得到一个密码加密后的密码 至少将密码进行两次这样的加密计算，如果用户名的长度是奇数，再进行一次加密，加密的过程只需要复制代码到 python 中，修改成 python 的格式就可以了。 步过了对密码的第一次加密后，继续步进上面设下的第二个断点 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465function strEnc(data,firstKey,secondKey,thirdKey)&#123; var leng = data.length;//取 data 的长度 var encData = \"\"; var firstKeyBt,secondKeyBt,thirdKeyBt,firstLength,secondLength,thirdLength; if(firstKey != null &amp;&amp; firstKey != \"\")&#123; firstKeyBt = getKeyBytes(firstKey);//取 firstkey 在每个字符之间插入一个字节的 0 firstLength = firstKeyBt.length;//取得插入 0 后的长度 &#125; if(secondKey != null &amp;&amp; secondKey != \"\")&#123; secondKeyBt = getKeyBytes(secondKey);//取 secondkey 在每个字符之间插入一个字节的 0 secondLength = secondKeyBt.length;//取得插入 0 后的长度 &#125; if(thirdKey != null &amp;&amp; thirdKey != \"\")&#123; //登陆过程中，并没用到 thirdkey，即 thirdKey = None thirdKeyBt = getKeyBytes(thirdKey);//取 thirdkey 在每个字符之间插入一个字节的 0 thirdLength = thirdKeyBt.length;//取得插入 0 后的长度 &#125; if(leng &gt; 0)&#123; if(leng &lt; 4)&#123; 如果 data 的长度＜4，因为跳过，代码用省略号替换 //省去一些代码…… &#125;else&#123; var iterator = parseInt(leng/4);//data 的长度除 64，得到循环次数 var remainder = leng%4; //data 的长度是否是 64 位的整数倍，保存余数 var i=0; for(i = 0;i &lt; iterator;i++)&#123; //开始循环 var tempData = data.substring(i*4+0,i*4+4); //循环取 data 的64 位 var tempByte = strToBt(tempData);//转换成 bits var encByte ; if(firstKey != null &amp;&amp; firstKey !=\"\" &amp;&amp; secondKey != null &amp;&amp; secondKey != \"\" )&#123; var tempBt; var x,y; tempBt = tempByte; for(x = 0;x &lt; firstLength ;x ++)&#123; tempBt = enc(tempBt,firstKeyBt[x]);//循环从firstkey 中取得64 位做密钥，依次对 data 中的某一段加密 &#125; for(y = 0;y &lt; secondLength ;y ++)&#123; tempBt = enc(tempBt,secondKeyBt[y]);//循环从second中取得64 位做密钥，依次对 data 中的某一段加密 &#125; encByte = tempBt;//保存加密结果 &#125; //………… if(remainder &gt; 0)&#123; //如果 data 有多余的长度，不足64 位 var remainderData = data.substring(iterator*4+0,leng); var tempByte = strToBt(remainderData);//将余下的分到4个16位的数组中 var encByte ; if(firstKey != null &amp;&amp; firstKey !=\"\" &amp;&amp; secondKey != null &amp;&amp; secondKey != \"\" &amp;&amp; thirdKey != null )&#123; var tempBt; var x,y,z; tempBt = tempByte; for(x = 0;x &lt; firstLength ;x ++)&#123; tempBt = enc(tempBt,firstKeyBt[x]);循环从firstkey 中取得64 位做密钥，依次对 data 中的某一段加密 &#125; for(y = 0;y &lt; secondLength ;y ++)&#123; tempBt = enc(tempBt,secondKeyBt[y]);循环从secondkey中取得64 位做密钥，依次对 data 中的某一段加密 &#125; encByte = tempBt;//保存加密结果 &#125; encData += bt64ToHex(encByte);//将加密后的文本转为16进制文本 &#125; &#125; &#125; return encData;//返回加密结果&#125; 这是一段循环进行 DES 加密的代码，先将data, firstkey, secondkey进行字符间插入一个字节的0, 然后不是 64 位整数倍长度的从上面代码看，相当于在后面补上 0 了从data中取出一段64位数据，循环用 firstkey 和 second 中的 64 位做密钥，层层加密，得到的结果和 data 中其他 64 位加密的结果串联后就是 req 的值了因为 key 都是 64 位的，再加上本身 sources 中也看到了 DES.js 文件，所以 enc(tempBt,secondkeyBt)应该就是 DES 算法了。但是自己写代码模拟登陆确发现结果和自己跟的结果不同，从代码中看，DES 采用了 ECB 模式，不是 CBC 模式，PAD_mode 也没问题，都64位，不需要 DES 自己填充啊。没办法，只得硬着头皮继续跟进 DES 加密的代码 我们知道，DES 加密需要先对 key 进行 置换，得到 56 位密钥，标准的 DES 都有个置换表，正常的 DES 置换表是这样的 1234567891011Permutation and translation tables for DES __pc1 = [56, 48, 40, 32, 24, 16, 8, 0, 57, 49, 41, 33, 25, 17, 9, 1, 58, 50, 42, 34, 26, 18, 10, 2, 59, 51, 43, 35, 62, 54, 46, 38, 30, 22, 14, 6, 61, 53, 45, 37, 29, 21, 13, 5, 60, 52, 44, 36, 28, 20, 12, 4, 27, 19, 11, 3 ] 即将 key 的第 56 位放到第 0 位，第 48 位放到第 1 位…………最后置换出 56 位的 key，再分成 2 个28 密钥，循环左移和右移，然后 对 IP 置换后的 data 加密，进行 Sbox 盒替换 和 Pbox 替换，再进行一次 IP-1 置换得到密文，解密算法一样。 但跟进 DES 加密函数没多久就发现问题了，找到密钥置换的函数 1var keys = generateKeys(keyByte); 并跟进： 123456789101112131415161718192021222324252627282930function generateKeys(keyByte)&#123; var key = new Array(56); var keys = new Array(); keys[ 0] = new Array(); keys[ 1] = new Array(); keys[ 2] = new Array(); keys[ 3] = new Array(); keys[ 4] = new Array(); keys[ 5] = new Array(); keys[ 6] = new Array(); keys[ 7] = new Array(); keys[ 8] = new Array(); keys[ 9] = new Array(); keys[10] = new Array(); keys[11] = new Array(); keys[12] = new Array(); keys[13] = new Array(); keys[14] = new Array(); keys[15] = new Array(); var loop = [1,1,2,2,2,2,2,2,1,2,2,2,2,2,2,1];//看到了循环移位的表，没看到置换表 for(i=0;i&lt;7;i++)&#123; for(j=0,k=7;j&lt;8;j++,k--)&#123; key[i*8+j]=keyByte[8*k+i];//用了这个循环生成 56 位 &#125; &#125;//省略代码&#125; 这里修改了标准的置换表，用了一个嵌套循环生成 56 位密钥，即把 原来 key 的 56 位 –&gt; 第 0 位，48 位 –&gt; 第 1 位，40 位 –&gt; 第 2 位，…………0 位–&gt; 第 7 位 原来 key 的 57 位 –&gt; 第 8 位，49 位 –&gt; 第 9 位，41 位 –&gt; 第 10 位，………… 1 位 –&gt;第 15 位 ………… 最后丢弃原 key 的第 63，55，47，39，31，23，15，7 位（位置号从 0 开始） 在 python 中不能直接使用标准的 DES库了，可以把标准库中的 pyDes.py 文件拷贝到工程同目录下，改名为 Des,py，并导入工程 from Des import * 另外在 Des.py 中找到 key 的置换表，修改成 123456789__pc1 = [ 56, 48, 40, 32, 24, 16, 8, 0, 57, 49, 41, 33, 25, 17, 9, 1, 58, 50, 42, 34, 26, 18, 10, 2, 59, 51, 43, 35, 27, 19, 11, 3, 60, 52, 44, 36, 28, 20, 12, 4, 61, 53, 45, 37, 29, 21, 13, 5, 62, 54, 46, 38, 30, 22, 14, 6 ] 就可以正常使用 Des 了 最后附上 python 代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146from Des import *from urllib.parse import quotefrom time import time, sleepfrom PIL import Imageimport requestsimport sysfrom bs4 import BeautifulSoup s = requests.session()headers = &#123; 'Cache-Control': 'max-age=0', 'Connection': 'keep-alive', 'Referer': 'http://www.jste.net.cn/uids/login.jsp', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) \\ Chrome/58.0.3029.110 Safari/537.36 SE 2.X MetaSr 1.0'&#125; def custom_encode(data): # 懒得注释了，直接从js中拷贝出来，改成python的代码 tab = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=' data_bytes = list(data.encode()) while len(data_bytes) % 3 != 0: data_bytes.append(0) b = 0 length = len(data_bytes) r = '' while b &lt; length: g = data_bytes[b] h = data_bytes[b + 1] j = data_bytes[b + 2] k = g &gt;&gt; 2 m = ((g &amp; 3) &lt;&lt; 4) | (h &gt;&gt; 4) n = ((h &amp; 15) &lt;&lt; 2) | (j &gt;&gt; 6) o = j &amp; 63 third_char = '=' if h == 0 else tab[n] fourth_char = '=' if j == 0 else tab[o] r = r + tab[k] + tab[m] + third_char + fourth_char b = b + 3 return r[::-1] # 反序输出 def encode_pwd(str_name, str_pwd): encoded_pwd = custom_encode(str_pwd) encoded_pwd = custom_encode(encoded_pwd) # 先连续对密码加密两次 if len(str_name) % 2 == 1: encoded_pwd = custom_encode(encoded_pwd) # 如果用户名长度是奇数，则再加密一次 return encoded_pwd def strenc(data, firstkey, secondkey): bts_data = extend_to_16bits(data) # 将data长度扩展成64位的倍数 bts_firstkey = extend_to_16bits(firstkey) # 将 first_key 长度扩展成64位的倍数 bts_secondkey = extend_to_16bits(secondkey) # 将 second_key 长度扩展成64位的倍数 i = 0 bts_result = [] while i &lt; len(bts_data): bts_temp = bts_data[i:i + 8] # 将data分成每64位一段，分段加密 j, k = 0, 0 while j &lt; len(bts_firstkey): des_k = des(bts_firstkey[j: j + 8], ECB) # 分别取出 first_key 的64位作为密钥 bts_temp = list(des_k.encrypt(bts_temp)) j += 8 while k &lt; len(bts_secondkey): des_k = des(bts_secondkey[k:k + 8], ECB) # 分别取出 second_key 的64位作为密钥 bts_temp = list(des_k.encrypt(bts_temp)) k += 8 bts_result.extend(bts_temp) i += 8 str_result = '' for each in bts_result: str_result += '%02X' % each # 分别加密data的各段，串联成字符串 return str_result def extend_to_16bits(data): # 将字符串的每个字符前插入 0，变成16位，并在后面补0，使其长度是64位整数倍 bts = data.encode() filled_bts = [] for each in bts: filled_bts.extend([0, each]) # 每个字符前插入 0 while len(filled_bts) % 8 != 0: # 长度扩展到8的倍数 filled_bts.append(0) # 不是8的倍数，后面添加0，便于DES加密时分组 return filled_bts def get_rand_code(): random_code_url = r'http://www.jste.net.cn/uids/genImageCode?rnd=' time_stamp = str(int(time() * 1000)) random_code_url += time_stamp try: req = s.get(random_code_url, headers=headers, stream=True) with open('rand_code.jpg', 'wb') as f: for chunk in req.iter_content(chunk_size=1024): f.write(chunk) except requests.RequestException: print('网络链接错误，请稍后重试/(ㄒoㄒ)/~~') sys.exit() with Image.open('rand_code.jpg')as img: img.show() def login_site(reqid, randomcode, reqkey): post_data = &#123; 'randomCode': randomcode, 'returnURL': None, 'appId': 'uids', 'site': None, 'encrypt': 1, 'reqId': reqid, 'req': reqkey &#125; try: req = s.post('http://www.jste.net.cn/uids/login.jsp', headers=headers, data=post_data) print('Status Code：%s' % req.status_code) # 不知道为什么浏览器上登陆成功返回的是302，这里返回200 if 'Set-Cookie' in req.headers.keys(): # 还好，看到response中出现Set-Cookie，就登陆成功了 return True else: return False except requests.RequestException: print('网络链接错误，请稍后重试/(ㄒoㄒ)/~~') return False def main(): print(''.center(100, '-')) uname = input('请输入你的用户名：') pwd = input('请输入你的登陆密码：') get_rand_code() secondkey = input('请输入看到的验证码：') # 取得验证码，作为second_key，提交数据时作为 randomCode 的值 firstkey = str(int(time() * 1000)) # 取得提交时的时间戳，作为first_key，提交数据时候作为 reqId 的值 crypt_pwd = encode_pwd(uname, pwd) # 对输入的密码进行第一次加密 data = quote(uname) + '\\n' + crypt_pwd # 用户名URI编码后和密码加密后的文本链接等待被DES加密 post_req = strenc(data, firstkey, secondkey) # 主要是DES计算，作为 req 的值提交数据 if login_site(reqid=firstkey, randomcode=secondkey, reqkey=post_req) is True: print(''.center(100, '-')) print('登陆成功，O(∩_∩)O哈哈~...') try: req = s.get('http://www.jste.net.cn/train/credit_hour/top.jsp') # 打开一个网页测试一下 soup = BeautifulSoup(req.text, 'html5lib') # 网页为多框架，测试下访问TOP框架中的文本 print(soup.select('.b')[0].text.replace('\\n', '').replace(' ', '')) except requests.RequestException: print('网络链接错误，请稍后重试/(ㄒoㄒ)/~~') if __name__ == '__main__': # 启动程序 main() 效果 最后思考了下，很多网站的数据都是明码提交的，或者是简单的加密提交的，这个网站在加密上花了一些工夫 但是js脚本最大的问题就是别人可以看到源码，虽然网站登陆成功后立即删除了js文件，但是只要出现了就会被发现，我网上搜索了下隐藏源码的办法，但是水平才菜了，没学过 java ，也没看懂。 最后补充下：DES加密的数据 data 是用户名的” URL格式 + 换行 + 密码第一次加密得到的文本“ firstkey 是提交时得到的时间戳，secondkey 就是输入的验证码","categories":[{"name":"python 爬虫 web安全","slug":"python-爬虫-web安全","permalink":"http://damiantuan.xyz/categories/python-爬虫-web安全/"}],"tags":[{"name":"python,爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"Hello World","slug":"hello-world","date":"2018-02-20T13:28:46.067Z","updated":"2017-11-18T07:52:41.738Z","comments":true,"path":"2018/02/20/hello-world/","link":"","permalink":"http://damiantuan.xyz/2018/02/20/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Startjust testCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server 1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"浅谈HTTP中Get与Post","slug":"浅谈HTTP中Get与Post","date":"2017-12-09T06:25:55.000Z","updated":"2017-12-09T08:13:31.272Z","comments":true,"path":"2017/12/09/浅谈HTTP中Get与Post/","link":"","permalink":"http://damiantuan.xyz/2017/12/09/浅谈HTTP中Get与Post/","excerpt":"前面的一部分是自己参考网上的一些比较浅显易懂的文章写的，也引用了一些段子；第二部分就是正儿八经的搞知识了。","text":"前面的一部分是自己参考网上的一些比较浅显易懂的文章写的，也引用了一些段子；第二部分就是正儿八经的搞知识了。 装逼部分GET和POST是HTTP请求的两种基本方法，要说它们的区别，接触过WEB开发的人都能说出一二。最直观的区别就是GET把参数包含在URL中，POST通过request body传递参数。其实网络传输方法有很多种，一个表格可以全部概述。 HTTPMethod RFC Request Has Body Response Has Body safe Idempotent Cacheable GET RFC7231 NO YES YES YES YES HEAD RFC7231 NO NO YES YES YES POST RFC7231 YES YES NO NO YES PUT RFC7231 YES YES NO YES NO DELETE RFC7231 NO YES NO YES NO CONNECT RFC7231 YES YES NO NO NO OPTIONS RFC7231 Optional YES YES YES NO TRACE RFC7231 NO YES YES YES NO PATCH RFC5789 YES YES NO NO YES 关于POST方法与GET方法的区别，很多教科书或是计算机类的书籍都会像下面这样写，恩，大致意思就是这样 GET在浏览器回退时是无害的，而POST会再次提交请求。 GET产生的URL地址可以被Bookmark，而POST不可以。 GET请求会被浏览器主动cache，而POST不会，除非手动设置。 GET请求只能进行url编码，而POST支持多种编码方式。 GET请求参数会被完整保留在浏览器历史记录里，而POST中的参数不会被保留。 GET请求在URL中传送的参数是有长度限制的，而POST么有。 对参数的数据类型，GET只接受ASCII字符，而POST没有限制。 GET比POST更不安全，因为参数直接暴露在URL上，所以不能用来传递敏感信息。 GET参数通过URL传递，POST放在Request body中。 然而这并不具有技术上的普适性，甚至有些说法还是错误的，比如说URL并没有长度的限制，URL长度的限制是与浏览器与系统有关的，其中所说的系统包括终端系统和服务端系统，URl在产生的时候其本身并没有任何的限制；再者来说安全性的问题，我只能说不考虑应用情景的所谓安全性都是在耍流氓。 首先我们要明白什么GET和POST。答案：HTTP协议中的两种发送请求的方法。说明白点，就是通信方式。比如A要联系B，那么A可以选择打电话或是发短信，微信和QQ，还可以写信，这其中不管那种方法都可以联系都B，只不过是形式不一样。HTTP是什么？答案：HTTP（超文本传输协议（HTTP，HyperText Transfer Protocol)）是基于TCP/IP的关于数据如何在万维网中如何通信的协议。这个有点像使用说明。互联网上面两台计算机互不相识，谁也不知道谁，HTTP中说明了一系列的东西，协议版本号，接受的字符类型，客户信息和内容之类的。就像你接到一分书信，一开始写明了是用中文书写的，你要找会中文的人，然后发现这种中文隶书写的，你还要从会中文的人中筛选出一个能看隶书的人来，然后······经过一系列的努力，这样到最后就可以解读出这份书信了。TCP/IP是什么？答案：Transmission Control Protocol/Internet Protocol的简写，中译名为传输控制协议/因特网互联协议，又名网络通讯协议，是Internet最基本的协议、Internet国际互联网络的基础，由网络层的IP协议和传输层的TCP协议组成。TCP/IP 定义了电子设备如何连入因特网，以及数据如何在它们之间传输的标准。协议采用了4层的层级结构，每一层都呼叫它的下一层所提供的协议来完成自己的需求。再说明白点，TCP/IP就是规定了互联网中计算机如何通信的协议，就像写信的时候是邮局告诉你规定邮政编码一样。 这样看，我们知道HTTP的底层是TCP/IP。所以GET和POST的底层也是TCP/IP，也就是说，GET/POST都是TCP链接。GET和POST能做的事情是一样一样的。你要给GET加上request body，给POST带上url参数，技术上是完全行的通的。 在万维网世界中，TCP就像汽车，我们用TCP来运输数据，它很可靠，从来不会发生丢件少件的现象。但是如果路上跑的全是看起来一模一样的汽车，那这个世界看起来是一团混乱，送急件的汽车可能被前面满载货物的汽车拦堵在路上，整个交通系统一定会瘫痪。为了避免这种情况发生，交通规则HTTP诞生了。HTTP给汽车运输设定了好几个服务类别，有GET, POST, PUT, DELETE等等，HTTP规定，当执行GET请求的时候，要给汽车贴上GET的标签(设置method为GET)，而且要求把传送的数据放在车顶上(url中)以方便记录。如果是POST请求，就要在车上贴上POST的标签，并把货物放在车厢里。当然，你也可以在GET的时候往车厢内偷偷藏点货物，但是这是很不光彩;也可以在POST的时候在车顶上也放一些数据，让人觉得傻乎乎的。HTTP只是个行为准则，而TCP才是GET和POST怎么实现的基本。 GET和POST还有一个重大区别，简单的说：GET产生一个TCP数据包;POST产生两个TCP数据包。长的说：对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200(返回数据);而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok(返回数据)。也就是说，GET只需要汽车跑一趟就把货送到了，而POST得跑两趟，第一趟，先去和服务器打个招呼“嗨，我等下要送一批货来，你们打开门迎接我”，然后再回头把货送过去。因为POST需要两步，时间上消耗的要多一点，看起来GET比POST更有效。因此Yahoo团队有推荐用GET替换POST来优化网站性能。但这是一个坑!跳入需谨慎。为什么? GET与POST都有自己的语义，不能随便混用。 据研究，在网络环境好的情况下，发一次包的时间和发两次包的时间差别基本可以无视。而在网络环境差的情况下，两次包的TCP在验证数据包完整性上，有非常大的优点。 并不是所有浏览器都会在POST中发送两次包，Firefox就只发送一次。很好，这就是装逼的第一步，完成了。 不装逼部分 第二部分引用陈曦明的文章，出于对知识的敬畏，不装逼了，认认真真搞。 Http定义了与服务器交互的不同方法，最基本的方法有4种，分别是GET，POST，PUT，DELETE。URL全称是资源描述符，我们可以这样认为：一个URL地址，它用于描述一个网络上的资源，而HTTP中的GET，POST，PUT，DELETE就对应着对这个资源的查，改，增，删4个操作。到这里，大家应该有个大概的了解了，GET一般用于获取/查询资源信息，而POST一般用于更新资源信息。 1.根据HTTP规范，GET用于信息获取，而且应该是安全的和幂等的。 (1).所谓安全的意味着该操作用于获取信息而非修改信息。换句话说，GET 请求一般不应产生副作用。就是说，它仅仅是获取资源信息，就像数据库查询一样，不会修改，增加数据，不会影响资源的状态。 * 注意：这里安全的含义仅仅是指是非修改信息。 (2).幂等的意味着对同一URL的多个请求应该返回同样的结果。这里我再解释一下幂等这个概念： 幂等（idempotent、idempotence）是一个数学或计算机学概念，常见于抽象代数中。 幂等有一下几种定义： 对于单目运算，如果一个运算对于在范围内的所有的一个数多次进行该运算所得的结果和进行一次该运算所得的结果是一样的，那么我们就称该运算是幂等的。比如绝对值运算就是一个例子，在实数集中，有abs(a)=abs(abs(a))。 对于双目运算，则要求当参与运算的两个值是等值的情况下，如果满足运算结果与参与运算的两个值相等，则称该运算幂等，如求两个数的最大值的函数，有在在实数集中幂等，即max(x,x) = x。看完上述解释后，应该可以理解GET幂等的含义了。 但在实际应用中，以上2条规定并没有这么严格。引用别人文章的例子：比如，新闻站点的头版不断更新。虽然第二次请求会返回不同的一批新闻，该操作仍然被认为是安全的和幂等的，因为它总是返回当前的新闻。从根本上说，如果目标是当用户打开一个链接时，他可以确信从自身的角度来看没有改变资源即可。 2.根据HTTP规范，POST表示可能修改变服务器上的资源的请求。继续引用上面的例子：还是新闻以网站为例，读者对新闻发表自己的评论应该通过POST实现，因为在评论提交后站点的资源已经不同了，或者说资源被修改了。 上面大概说了一下HTTP规范中GET和POST的一些原理性的问题。但在实际的做的时候，很多人却没有按照HTTP规范去做，导致这个问题的原因有很多，比如说： 1.很多人贪方便，更新资源时用了GET，因为用POST必须要到FORM（表单），这样会麻烦一点。 2.对资源的增，删，改，查操作，其实都可以通过GET/POST完成，不需要用到PUT和DELETE。 3.另外一个是，早期的Web MVC框架设计者们并没有有意识地将URL当作抽象的资源来看待和设计，所以导致一个比较严重的问题是传统的Web MVC框架基本上都只支持GET和POST两种HTTP方法，而不支持PUT和DELETE方法。 * 简单解释一下MVC：MVC本来是存在于Desktop程序中的，M是指数据模型，V是指用户界面，C则是控制器。使用MVC的目的是将M和V的实现代码分离，从而使同一个程序可以使用不同的表现形式。 以上3点典型地描述了老一套的风格（没有严格遵守HTTP规范），随着架构的发展，现在出现REST(Representational State Transfer)，一套支持HTTP规范的新风格，这里不多说了，可以参考《RESTful Web Services》。 说完原理性的问题，我们再从表面现像上面看看GET和POST的区别： 1.GET请求的数据会附在URL之后（就是把数据放置在HTTP协议头中），以?分割URL和传输数据，参数之间以&amp;相连，如：login.action?name=hyddd&amp;password=idontknow&amp;verify=%E4%BD%A0%E5%A5%BD。如果数据是英文字母/数字，原样发送，如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64加密，得出如：%E4%BD%A0%E5%A5%BD，其中％XX中的XX为该符号以16进制表示的ASCII。 POST把提交的数据则放置在是HTTP包的包体中。 2.”GET方式提交的数据最多只能是1024字节，理论上POST没有限制，可传较大量的数据，IIS4中最大为80KB，IIS5中为100KB”？？！ 以上这句是我从其他文章转过来的，其实这样说是错误的，不准确的： (1).首先是”GET方式提交的数据最多只能是1024字节”，因为GET是通过URL提交数据，那么GET可提交的数据量就跟URL的长度有直接关系了。而实际上，URL不存在参数上限的问题，HTTP协议规范没有对URL长度进行限制。这个限制是特定的浏览器及服务器对它的限制。IE对URL长度的限制是2083字节(2K+35)。对于其他浏览器，如Netscape、FireFox等，理论上没有长度限制，其限制取决于操作系统的支持。 注意这是限制是整个URL长度，而不仅仅是你的参数值数据长度。[见参考资料5] (2).理论上讲，POST是没有大小限制的，HTTP协议规范也没有进行大小限制，说“POST数据量存在80K/100K的大小限制”是不准确的，POST数据是没有限制的，起限制作用的是服务器的处理程序的处理能力。 对于ASP程序，Request对象处理每个表单域时存在100K的数据长度限制。但如果使用Request.BinaryRead则没有这个限制。 由这个延伸出去，对于IIS 6.0，微软出于安全考虑，加大了限制。我们还需要注意： 1).IIS 6.0默认ASP POST数据量最大为200KB，每个表单域限制是100KB。 2).IIS 6.0默认上传文件的最大大小是4MB。 3).IIS 6.0默认最大请求头是16KB。 IIS 6.0之前没有这些限制。[见参考资料5] 所以上面的80K，100K可能只是默认值而已(注：关于IIS4和IIS5的参数，我还没有确认)，但肯定是可以自己设置的。由于每个版本的IIS对这些参数的默认值都不一样，具体请参考相关的IIS配置文档。 3.在ASP中，服务端获取GET请求参数用Request.QueryString，获取POST请求参数用Request.Form。在JSP中，用request.getParameter(\\”XXXX\\”)来获取，虽然jsp中也有request.getQueryString()方法，但使用起来比较麻烦，比如：传一个test.jsp?name=hyddd&amp;password=hyddd，用request.getQueryString()得到的是：name=hyddd&amp;password=hyddd。在PHP中，可以用\\$_GET和\\$_POST分别获取GET和POST中的数据，而\\$_REQUEST则可以获取GET和POST两种请求中的数据。值得注意的是，JSP中使用request和PHP中使用$_REQUEST都会有隐患，这个下次再写个文章总结。 4.POST的安全性要比GET的安全性高。注意：这里所说的安全性和上面GET提到的“安全”不是同个概念。上面“安全”的含义仅仅是不作数据修改，而这里安全的含义是真正的Security的含义，比如：通过GET提交数据，用户名和密码将明文出现在URL上，因为(1)登录页面有可能被浏览器缓存，(2)其他人查看浏览器的历史纪录，那么别人就可以拿到你的账号和密码了，除此之外，使用GET提交数据还可能会造成Cross-site request forgery攻击。 总结一下，Get是向服务器发索取数据的一种请求，而Post是向服务器提交数据的一种请求，在FORM（表单）中，Method默认为”GET”，实质上，GET和POST只是发送机制不同，并不是一个取一个发！ 参考资料： [1].http://hi.baidu.com/liuzd003/blog/item/7bfecbfa6ea94ed8b58f318c.html [2].http://www.blogjava.net/onlykeke/archive/2006/08/23/65285.aspx [3].http://baike.baidu.com/view/2067025.htm [4].http://www.chxwei.com/article.asp?id=373 [5].http://blog.csdn.net/somat/archive/2004/10/29/158707.aspx 引用地址：https://www.cnblogs.com/hyddd/archive/2009/03/31/1426026.html","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://damiantuan.xyz/categories/爬虫/"}],"tags":[{"name":"网络","slug":"网络","permalink":"http://damiantuan.xyz/tags/网络/"}]},{"title":"网页状态码及其处理","slug":"网页状态码及其处理","date":"2017-12-03T09:55:21.000Z","updated":"2017-12-03T10:03:35.516Z","comments":true,"path":"2017/12/03/网页状态码及其处理/","link":"","permalink":"http://damiantuan.xyz/2017/12/03/网页状态码及其处理/","excerpt":"记录一下在学习爬虫过程中遇到的几种网页的状态码以及处理方法","text":"记录一下在学习爬虫过程中遇到的几种网页的状态码以及处理方法 写几个主要的，像200这样访问成功的状态码就没有必要写下来了。 状态码及其含义 400 Bad Request 客户端请求有语法错误，不能被服务器所理解 401 Unauthorized 请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 403 Forbidden 服务器收到请求，但是拒绝提供服务 404 Not Found 请求资源不存在，eg：输入了错误的URL 500 Internal Server Error 服务器发生不可预期的错误 503 Server Unavailable 服务器当前不能处理客户端的请求，一段时间后可能恢复正常 处理方法400 Bad Request - 检查请求的参数或者路径 401 Unauthorized - 如果需要授权的网页，尝试重新登录 403 Forbidden 如果是需要登录的网站，尝试重新登录 IP被封，暂停爬取，并增加爬虫的等待时间，如果拨号网络，尝试重新联网更改IP 404 - Not Found 直接丢弃5XX - 服务器错误，直接丢弃，并计数，如果连续不成功，WARNING 并停止爬取","categories":[{"name":"python,爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/categories/python-爬虫/"}],"tags":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/tags/python爬虫/"}]},{"title":"计算上证日K与个股的相关性系数","slug":"计算上证日K与个股的相关性系数","date":"2017-11-21T07:06:05.000Z","updated":"2017-11-21T08:30:10.476Z","comments":true,"path":"2017/11/21/计算上证日K与个股的相关性系数/","link":"","permalink":"http://damiantuan.xyz/2017/11/21/计算上证日K与个股的相关性系数/","excerpt":"这算是一个面试的题吧。需求是把与上证日K趋势的相关性系数大于80%的个股挑选出来。","text":"这算是一个面试的题吧。需求是把与上证日K趋势的相关性系数大于80%的个股挑选出来。这是一个面试题，当时面试一个做量化交易的公司的一个面试题，不过对方对我这个脚本似乎不太满意，面试的后续就没有后续了。 思路数据获取 在上交所正常盘面交易的股票大约有3400多只，如果写爬虫去获取每一只股票的相关信息那么工程量肯定不小，当然加上现在各个金融网站的反爬手段，单单写爬虫这点可能都会卡住，就在网上看看有没有什么免费的API可以使用，无意间找到了tushare这个模块，全中文手册，使用全免费，这个当然是不错的。后面在使用的过程中发现在获取当日交易情况的时候会出现不稳定然后中断获取的情况，这个······毕竟免费的而且没有像各个官方的限制，其实还是很不错的，而且也提供了大量的方法可以使用。 数据处理 在获取完数据之后应该对数据进行处理，我查看了一下tushare输出的数据类型，基本上都是DataFormat的类型，这点处理上是比较有利的，但是考虑到tushare在获取数据的时候的稳定性情况，我决定不直接使用网络数据，转而使用把数据保存到本地后再使用，在使用完成后我们可以对本地文件进行删除。在这里我不考虑对每一项数据都进行入库处理，因为tushare提供了一个将数据保存的本地的方法，使用起来也十分方便。 提取结果 每只股票的代码是6位数，相关性系数检验在80%以上的应该不会很多，所以我采用直接将股票代码保存在一个txt文本文件中。 方法获取有效股票代码 选择使用tushre之后，知道了如何获取个股的数据和上证指数，但是模块中没有提供我想要的单单的股票代码，我也不能用循环去生成股票代码，虽然说股票的代码确实是有顺序的，但是不敢保证生成的股票代码的那只股票现在是否还在市。想想，如果可以获取当日的交易的所有股票行情的话这个问题不就解决了，因为当日还在交易的所有股票肯定还没有退市的。查一下手册，发现有一个函数是一次性获取当前交易所有股票的行情数据，ok就是他了。获取到了之后保存为本地excel表格，再从表格中读取股票代码。 计算K线趋势 后面发现使用tushare获取的数据并没有计算出当日的K线的走势，不过没关系了，他给出的数据中有一天的开盘价，收盘价，最高价和最低价，这些足够计算当日的K线了。正值为红，负值为绿。在计算的过程中发现读出来的数据是一个列表，其中包括表头的值，于是取下标从1开始。列表的计算在python并不是很方便，因为python并没有像c一样提供一个数组的数据结构，幸好前几天刚刚学习了numpy模块，现学现卖了，把读取出来的列表通过numpy模块转换成数组，再使用numpy模块内置算法计算，这样大大的减少了工作量。 相关性系数 Pearson系数也称为是简单相关性系数，但是计算中要求两个计算的量近似服从二维的正态分布，而上证K线的趋势和个股K线的趋势其实并没有什么规律，不能使用这个，于是采用Spearman系数，Spearman系数在计算的过程中并不要求计算的两组数字的分布规律。 工作流程 工作流程就是获取上证指数的信息，保存为表格，读表，计算K线，获取当日交易情况，保存为表格，读表，处理，得出个股编号，再通过编号去获取该股票的历史数据，保存为表格，读表，计算K线，再把个股的K线与上证的K线从获取之日开始计算，得出Spearman系数，最后把大于Spearman系数大于80%的个股编号保存下来，不满足条件的直接忽略。最后计算完该股票后把保存该股票信息的excel表格删掉，减少占用磁盘空间。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# *-*coding='UTF-8'*-*import tushare as tsimport numpy as npimport xlrdimport pandasimport timeimport os# 获取上证指数的K线def getshangzheng_K(): #读取上证指数的日K shangzheng = ts.get_hist_data('sh',ktype='D',start='2017-07-01') # 将上证指数的日K存入excel表格 shangzheng.to_excel('sh.xlsx',sheet_name='Sheet1') (filepath,tempfilename)=os.path.split('sh.xlsx') return tempfilename# 读取excel表格def read_excel(fileName='',sheet='Sheet1'): workbook=xlrd.open_workbook(fileName) sheet1=workbook.sheet_by_name(sheet) return sheet1# 计算K线的趋势def K(sheet,open_col_values=False,close_col_values=False): open_price=sheet.col_values(open_col_values) close_price=sheet.col_values(close_col_values) open_price_array=np.array(open_price[1:],dtype=float) close_price_array=np.array(close_price[1:],dtype=float) movements=close_price_array-open_price_array the_k=pandas.Series(movements) return the_k# 获取上证交易所中当日正在交易的所有股票信息def get_today_stock(): todayall=ts.get_today_all() todayall.to_excel('todays.xlsx') (filepath,tempfilename)=os.path.split('todays.xlsx') return tempfilename# 获取上证中正在交易的股票的代码def get_stock_num(sheet=''): todaycols=sheet.col_values(1) return todaycols# 获取个股的详细信息def get_stock_info(stock_num): stock_number=str(stock_num) stock_info=ts.get_hist_data(stock_number,start='2017-07-01',ktype='D') stock_info.to_excel(stock_number+'.xlsx') (filepath,tempfilename)=os.path.split(stock_number+'.xlsx') return tempfilename# 计算相关性系数，因为数据分布无规则，所以计算spearman相关系数def find_spearman(sh,stock): corr_spearman=sh.corr(stock,method='spearman') return corr_spearmanif __name__=='__main__': #获取上证信息并保存为表格 sh_excel=getshangzheng_K() #读取上证信息表格 sh_sheet=read_excel(sh_excel,sheet='Sheet1') # 计算上证日K的趋势 sh_K=K(sh_sheet,open_col_values=1,close_col_values=3) # 计算个股的日K趋势 today_stock_exccel=get_today_stock() #实际使用中，read_excel函数的第一个参数可以直接写成文件名，这样不必每次执行都重复去获取数据 today_sheet=read_excel(today_stock_exccel,sheet='Sheet1') stock_nums=get_stock_num(today_sheet) for stock_num in stock_nums[1:]: stock_excel=get_stock_info(stock_num) stock_sheet=read_excel(fileName=stock_excel,sheet='Sheet1') stock_K=K(sheet=stock_sheet,open_col_values=1,close_col_values=3) #以上为计算个股的K线趋势 #此处计算Spearman系数 spearman=find_spearman(sh_K,stock_K) if spearman&gt;0.800: with open('save.txt','a+') as fp: fp.write(str(stock_num).encode('utf8') + '\\t') print 'done one',time.strftime(\"%Y-%m-%d %X\", time.localtime()) else: print 'let one go',time.strftime(\"%Y-%m-%d %X\", time.localtime()) #这句为删除使用过的表格，如果想保留相关表格，可以将其注释掉 os.remove(str(stock_num)+'.xlsx') 在实际使用的过程中我们可以将上证的表格和当日成交的表格给保留下来，并在代码中注释掉相应的执行语句，将文件名直接填成参数，这样可以避免获取数据的时候tushare的不稳定造成的影响。 最后，计算的结果为300719 300716 002781 000555","categories":[{"name":"量化分析","slug":"量化分析","permalink":"http://damiantuan.xyz/categories/量化分析/"}],"tags":[{"name":"-python -量化分析","slug":"python-量化分析","permalink":"http://damiantuan.xyz/tags/python-量化分析/"}]},{"title":"scrapy爬取当当网所有3c产品商品名称，价格，评论数","slug":"scrapy爬取当当网所有3c产品商品名称，价格，评论数","date":"2017-11-19T08:31:19.000Z","updated":"2017-11-19T09:38:31.784Z","comments":true,"path":"2017/11/19/scrapy爬取当当网所有3c产品商品名称，价格，评论数/","link":"","permalink":"http://damiantuan.xyz/2017/11/19/scrapy爬取当当网所有3c产品商品名称，价格，评论数/","excerpt":"Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。Scrapy 使用 Twisted这个异步网络库来处理网络通讯，架构清晰，并且包含了各种中间件接口，可以灵活的完成各种需求。","text":"Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。Scrapy 使用 Twisted这个异步网络库来处理网络通讯，架构清晰，并且包含了各种中间件接口，可以灵活的完成各种需求。 Scrapy 首先祭上scrapy的工作流程图。 我们暂时不去管scrapy的设计，先看看怎么用先。 在命令终端输入123scrapy startproject dangdangcd dangdangscrapy genspider dangdangwang dangdang.com 解释一下上面的命令的含义，startporject是新建一个工程，cd就是到新建的工程项目文件夹下，genspider是依照模板生成一个爬虫，默认状态是basic模板。 在工程文件夹下的终端输入tree就可以得到如下的文件，如果输入tree显示找不到该命令的话，先安装一个小软件，apt-get install tree ,附上tree的手册tree(1) - Linux man page1234567891011121314151617181920tree .├── 201708260.txt├── dangdang│ ├── __init__.py│ ├── __init__.pyc│ ├── items.py│ ├── items.pyc│ ├── middlewares.py│ ├── pipelines2excel.py│ ├── pipelines.py│ ├── pipelines.pyc│ ├── settings.py│ ├── settings.pyc│ └── spiders│ ├── dangdangwang.py│ ├── dangdangwang.pyc│ ├── __init__.py│ └── __init__.pyc└── scrapy.cfg 文件布局如上面树所示 开始工作修改item.py文件 item.py是定义scrapy抓取信息的地方，相当与你在这里要为你想得到的每一种信息都要取一个名字，名字当然是见字知意最好了。 12345678910111213141516171819# -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# http://doc.scrapy.org/en/latest/topics/items.htmlimport scrapyclass DangdangItem(scrapy.Item): # define the fields for your item here like: # name = scrapy.Field() # 商品名称 title=scrapy.Field() # 评论数 num=scrapy.Field() # 价格 price=scrapy.Field() # 页面链接 link=scrapy.Field() 修改dangdangwang.py文件 dangdangwang.py文件位于spiders文件夹下，是爬虫的主要行为，他定义了如何去定位信息，把什么信息赋值给哪个item。 123456789101112131415161718192021222324252627# -*- coding: utf-8 -*-import scrapyfrom dangdang.items import DangdangItemclass DangdangwangSpider(scrapy.Spider): name = \"dangdangwang\" allowed_domains = [\"dangdang.com\"] start_urls = ['http://category.dangdang.com/pg1-cid4002590.html'] def parse(self, response): # 每一个商品的点击链接（详情链接） url_list=response.xpath('.//*[@id=\"component_0__0__8609\"]/li/p[@class=\"name\"]/a/@href').extract() for url in url_list: yield scrapy.http.Request(url,callback=self.parse_name) # 搜索分类的分页 for i in range(2,98): page_url='http://category.dangdang.com/pg&#123;&#125;-cid4002590.html'.format(i) yield scrapy.http.Request(page_url,callback=self.parse) def parse_name(self,response): items=DangdangItem() items['title']=response.xpath('//div[@class=\"name_info\"]/h1/text()').extract() items['num']=response.xpath('//div[@class=\"pinglun\"]/a/text()').extract() items['price']=response.xpath('//div[@class=\"price_d\"]/p[@id=\"dd-price\"]/text()').extract() items['link']=response.url yield items 修改pipelines.py pipelines.py文件是定义了爬取的数据如何保存的文件，就像他的名字一样，数据的管道。 123456789101112131415161718192021222324# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.htmlimport timeimport os.pathclass DangdangPipeline(object): def process_item(self, item, spider): # print u\"商品名字：\"+item['title'][0] # print u\"商品价格：\"+item['price'][0] # print u\"商品评论数\"+item['num'][0] # return item today = time.strftime('%Y%m%d', time.localtime()) fileName = today + '.txt' with open(fileName,'a') as fp: fp.write(item['title'][0].replace(' ','').encode('utf8') + '\\t') fp.write(\"价格\"+item['price'][0].encode('utf8') + '\\t') fp.write(\"评论数\"+item['num'][0].encode('utf8') + '\\t') fp.write('\\n') # time.sleep(1) return item 这里定义的是将数据保存到一个txt记事本。效果如图所示","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"-python -爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"py+bs+requests多线程爬虫","slug":"py-bs-requests多线程爬虫","date":"2017-11-19T08:09:30.000Z","updated":"2017-11-19T08:15:21.921Z","comments":true,"path":"2017/11/19/py-bs-requests多线程爬虫/","link":"","permalink":"http://damiantuan.xyz/2017/11/19/py-bs-requests多线程爬虫/","excerpt":"还是那个站，不过我们这次不再一个一个来爬取了，我们将采用多线程进行爬取。","text":"还是那个站，不过我们这次不再一个一个来爬取了，我们将采用多线程进行爬取。 python的多线程问题一直是一个备受争议的话题，因为在多核心CPU的硬件条件下，python依然还是只能利用单核心，所以python的多线程就有一个“伪多线程”的命题，但是python多线程虽然是只能利用单核心，但是依旧比单线程要快。 思路还是一样，不过在最后处理的时候我们引入了一个多线程。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#-*-coding:utf8-*-#!usr/bin/pythonimport requestsimport timefrom multiprocessing.dummy import Pool as ThreadPoolfrom bs4 import BeautifulSoupheader = &#123;'User-Agent':'\"Mozilla/5.0 (X11; Linux x86_64; rv:45.0) Gecko/20100101 Firefox/45.0\"'&#125;# 读取网页源码def getHtml(url): htmls=requests.get(url,headers=header) # 发现网页是用GBK编码的，在此处进行转码 htmls.encoding = 'gb2312' # 调用text将对象进行字符化 pageContent=htmls.text return pageContent# 进行文章url的获取def getContentUrl(html): urls=[] bsContent=BeautifulSoup(html,'html.parser') urlContent=bsContent.find(class_=\"liszw\") for link in urlContent.find_all('a'): url_lib=link.get('href') urls.append(url_lib) return urls#文章内容的获取def readContent(urls): articleHtml=getHtml(urls) # print articleHtml bsContent=BeautifulSoup(articleHtml,'html.parser') title=bsContent.find('h2').string content=bsContent.find(class_=\"arwzks\") article=content.get_text() txt=article.encode('utf-8') print title+' start' open(r'/home/wukong/testTuiLiXue/download/'+title.encode(\"utf-8\")+'.txt','w+').write(txt) print time.strftime('%Y-%m-%d %X',time.localtime(time.time()))+title+' end' # print txt return txtif __name__ == '__main__': #后面的参数为CPU的核心数，虽然说只能利用单核心 pool = ThreadPool(4) links=[] for i in range(1,40): links.append('http://tuilixue.com/zhentantuilizhishi/list_4_'+str(i)+'.html') for link in links: html=getHtml(link) urls=getContentUrl(html) url=[] for i in range(0,20,2): url.append(urls[i]) # for i in url: # result = readContent(i) result=pool.map(readContent,url) pool.close() pool.join()","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"-python -爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"BeautifulSoup爬虫代码","slug":"BeautifulSoup爬虫代码","date":"2017-11-19T07:40:14.000Z","updated":"2017-11-19T08:01:54.245Z","comments":true,"path":"2017/11/19/BeautifulSoup爬虫代码/","link":"","permalink":"http://damiantuan.xyz/2017/11/19/BeautifulSoup爬虫代码/","excerpt":"还是上次的那个网站，不过这次我们用request+beautifulsoup来进行爬取了。","text":"还是上次的那个网站，不过这次我们用request+beautifulsoup来进行爬取了。 思路和上次的那个基本上是一样的，不过就是把定位信息的方法从原来的使用python内置的str模块中的函数方法改成了使用beautifulsoup这个第三方的模块，这个模块的手册在网上能找到，翻译得不错，基本上是一看就懂的那种。注释的话我这次没写了，因为和上次一样的，想看注释的可以去看上一篇文章。 这次的存储方法与上次使用urllib的有所不同，上次的存储是直接保存HTML文件的要使用一些处理结构性文档的工具才能查看文章的内容，而且文件命名也是使用的网站上的URL来进行的，这样的命名毫无意义也就无法知道文件中的内容是什么，所以这次我们把爬取的文章标题作为文件名，保存为txt记事本文件。 1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/usr/bin/python#coding=utf-8import requestsimport timefrom bs4 import BeautifulSoupurl=['']*20header = &#123;'User-Agent':'\"Mozilla/5.0 (X11; Linux x86_64; rv:45.0) Gecko/20100101 Firefox/45.0\"'&#125;for page in range(1,39): htmls=requests.get('http://tuilixue.com/zhentantuilizhishi/list_4_'+str(page)+'.html',headers=header) htmls.encoding = 'gb2312' pageContent=htmls.text txt=pageContent# print type(txt)# open(r'/home/wukong/testTuiLiXue/download/123.txt','w+').write(txt.encode('utf-8'))# print content bsContent=BeautifulSoup(pageContent,'html.parser') urlContent=bsContent.find(class_=\"liszw\") lis=urlContent.find_all('a') lis=str(lis) hrefHeader=lis.find(r'href=') hrefTrail=lis.find(r'target=\"_blank\"&gt;',hrefHeader) url[0]=lis[hrefHeader+6:hrefTrail-2] if hrefHeader!=-1 and hrefTrail!=-1: for times in range(1,20): hrefHeader=lis.find(r'href=',hrefTrail) hrefTrail=lis.find(r'target=\"_blank\"&gt;',hrefHeader) url[times]=lis[hrefHeader+6:hrefTrail-2] # print url[i] for i in range(0,20,2): articleHtml=requests.get(url[i],headers=header) articleHtml.encoding='gb2312' articleContent=articleHtml.text bsContent=BeautifulSoup(articleContent,'html.parser') title=bsContent.find('h2').string content=bsContent.find(class_=\"arwzks\") article=content.get_text() txt=article.encode('utf-8') print title+' start' # print txt open(r'/home/wukong/testTuiLiXue/download/'+title.encode(\"utf-8\")+'.txt','w+').write(txt) print title+' end' time.sleep(1)print 'finish'","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"-python -爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"三个步骤搞定一个爬虫--源码","slug":"三个步骤搞定一个爬虫-源码","date":"2017-11-19T07:18:57.000Z","updated":"2017-11-19T07:29:34.969Z","comments":true,"path":"2017/11/19/三个步骤搞定一个爬虫-源码/","link":"","permalink":"http://damiantuan.xyz/2017/11/19/三个步骤搞定一个爬虫-源码/","excerpt":"","text":"这里给出“三个步骤搞定一个爬虫的python源码” 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#-*-coding:utf8-*-#!usr/bin/pythonimport urllibimport timeurl=['']*10title=['']*10# 循环页数for page in range(1,39): pageContent=urllib.urlopen('http://tuilixue.com/zhentantuilizhishi/list_4_'+str(page)+'.html').read() # print pageContent #这一部分先找到某一页中的第一篇文章作为初始化 #根据网页代码特性，发现只有需要的页面文章链接前面有/span&gt;&lt;a href=，于是找到页面中的/span&gt;&lt;a href=的下标开始 hrefHeader=pageContent.find(r'/span&gt;&lt;a href=') # print hrefHeader #发现所需文章链接后面没有唯一的特性，于是从前面找到的下标开始找第一个target=\"_blank\"&gt;的下标 hrefTrail=pageContent.find(r'target=\"_blank\"&gt;',hrefHeader) # print hrefTrail #根据找到的下标，增加一定的位数找到文章链接，就是一个切片操作 url[0]=pageContent[hrefHeader+15:hrefTrail-2] #寻找文章标题 titleHead=pageContent.find(r'target=\"_blank\"&gt;',hrefHeader) titleTrail=pageContent.find(r'&lt;/a&gt;',titleHead) title[0]=pageContent[titleHead+16:titleTrail-1] if hrefHeader!=-1 and hrefTrail!=-1: for times in range(1,10): # 从前面的找到的尾部下标开始寻找 hrefHeader=pageContent.find(r'/span&gt;&lt;a href=',hrefTrail) #从上一步开始寻找 hrefTrail=pageContent.find(r'target=\"_blank\"&gt;',hrefHeader) #根据找到的下标，增加一定的位数找到文章链接，就是一个切片操作 url[times]=pageContent[hrefHeader+15:hrefTrail-2] #寻找文章标题 titleHead=pageContent.find(r'target=\"_blank\"&gt;',hrefHeader) titleTrail=pageContent.find(r'&lt;/a&gt;',titleHead) title[times]=pageContent[titleHead+16:titleTrail-1] for times in range(0,10): #分别读出每一个文章的url articleContent=urllib.urlopen(url[times]).read() print \"Start download~~\"+str(url[times][-9:]) #以html的格式保存 open(r'/home/wukong/testTuiLiXue/download/'+url[times][-9:].replace('/',''),'w+').write(articleContent) print \"Download finish\"+str(page-1)+str(times) #缓存时间为1s time.sleep(1)print \"All finish\"","categories":[],"tags":[]},{"title":"三个步骤搞定一个爬虫（3）","slug":"三个步骤搞定一个爬虫（3）","date":"2017-11-19T05:32:50.000Z","updated":"2017-11-19T05:52:22.614Z","comments":true,"path":"2017/11/19/三个步骤搞定一个爬虫（3）/","link":"","permalink":"http://damiantuan.xyz/2017/11/19/三个步骤搞定一个爬虫（3）/","excerpt":"上两篇文章讲了怎么把整页的文章下载下来，这一次讲一下怎么把整个专栏的文章都下载下来。讲到这里，一个基础的爬虫基本上都完成了，所以技术方面这些东西并不是有什么神秘的，只是很多东西没有学到，没有思路就会觉得不可思议而已。","text":"上两篇文章讲了怎么把整页的文章下载下来，这一次讲一下怎么把整个专栏的文章都下载下来。讲到这里，一个基础的爬虫基本上都完成了，所以技术方面这些东西并不是有什么神秘的，只是很多东西没有学到，没有思路就会觉得不可思议而已。 在我们开始之前我们先把上次遗留下来的问题解决一下。大家想一下，我们在之前是直接就去循环文章的链接了，尽管我们是根据html文档来进行分析判断的，但是似乎还是不够严谨，因为万一我们的判断失误了呢，那么程序就会没有响应并且一直卡在那里，因此我们应该加一个判断。 因为我们知道find函数在没有找到目标字符的时候返回-1,所以我们应该判断在不返回-1的情况下才对链接进行循环遍历。 开始工作分析源码 好了，这次我们是要下载整个推理讨论板块的所有文章，我们还是对网站进行分析先。 这是第一页的网址，我们再来看看第二页 再来看看第三页 很好，直觉告诉我们，最后的那个数字应该就是页数的意思了，我们试试改成1看看能不能回到第一页。 成功的回到了第一页，那么我们来看看需要循环几次 一共有38页，372篇文章，我们先来写一个循环看看能不能正确循环出页数 因为字符串是不能和整型相加的，并且urllib.openg也只能接受字符参数，所以我们这里用str（）把页数的数字转换为字符型，并且连接在网址上。测试一下 篇幅原因我就不全部截图了，但是我们可以看到，这个循环应该是可以把所有的页数都正确的表示出来的，那么我们现在就要把这个循环语句加到程序中去了。大家想想，我们应该是先到一个具体的页数，然后再爬取该页的链接和文章吧，所以我这个地方的循环应该是加在最外面的一个循环。 这样我们的爬虫基本就完工了，下面我们来做一些其他的事情。首先是对于网站的压力应对，我们现在写的是一个小爬虫，对网站不能造成什么压力，而且是单线爬虫，如果以后写多线程和爬取的内容多的时候，难免造成对网站的请求过于平凡，很多时候爬虫就会被服务器封杀掉，这时我们就要限制爬虫的速度了。我们先来导入一个模块 然后在每次下载完成的时候让程序暂停一下，我这里设置的是1秒钟 然后我们再来看看我们的爬虫，大家不知道有没有发现，我们下载的文章的保存名字似乎是一个递增的数字，那么我猜想可能是这个网站累计文章的篇数，为了证实这个猜想，我们到最后几页去看看。我们先来看看37页的html代码. 这是我们发现我们用来命名文件的数字由4位数变成了3位数，又变成了2位数，我们这个时候在来看看我们的下载保存路径。 这时我们要注意到，如果文章命名的数字变成2位数的时候，命名的切片操作会把文章链接中的/符号也加进去，这时就会构成一个新的下载路径了，由于我们并没有这样的路径，所有程序就会报错。具体错误是没有一个这样的文件指向。这个是怎么样的情形大家可以自己去实验一下。下面我们就要解决这个问题，想办法把/替换掉或者去掉。我们当然可以用find找出最后的/的下标，再从下标加1的地方去执行命名，但是这样无疑会多写出几行代码来，而且每次都要去判断，之后命名的代码也要重写，这样似乎工作量就上去了。我们可以换一个想法来解决这个问题，比如说在原有的命名代码中，只要发现有/字符存在的，一律替换为空或者其他。下面我们来看一个函数。 字符串的replace函数可以将字符串中的目标字符替换成其他的字符，这时我们来这样写。 就这样，我们就可以轻松的把我们切片中的/给去掉了我们运行一下看看是否可以下载。先看看目录下面有什么。 这些都是上一次我们下载的东西，我们删除掉。 现在目录已经是空的了，我们现在开始下载试试。 一堆end之后我们的文件下载好了，我们去看看. 一大堆的文件，我们看看数量 372个，刚好对应了372篇文章。就这样，我们就完成了一个定制的爬虫。","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"python,爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"三个步骤搞定一个爬虫（2）","slug":"三个步骤搞定一个爬虫（2）","date":"2017-11-19T02:35:23.000Z","updated":"2017-11-19T05:30:09.210Z","comments":true,"path":"2017/11/19/三个步骤搞定一个爬虫（2）/","link":"","permalink":"http://damiantuan.xyz/2017/11/19/三个步骤搞定一个爬虫（2）/","excerpt":"第一篇文章写了怎么去获取一篇文章，这一篇文章就写写怎么把一页中的10篇文章全部获取到。","text":"第一篇文章写了怎么去获取一篇文章，这一篇文章就写写怎么把一页中的10篇文章全部获取到。 开始工作获取其他链接 上一次我说了怎么去获取第一条的文章链接，现在我们再来爬取本页后面剩下的链接。我们先来看看上次我们爬取链接用的代码。 获取后面的链接我们能不能如法炮制呢，我们先来试试。我们把代码写成下面那样 然后我们现在来试试 结果我们发现我们试图获取的三条链接都是一样的，可以看出，这还是本页的第一篇文章的链接。证明我们这种方法是不可行的。我们回想一下上一节课我们讲的定位链接使用函数。 就是这个find函数，我们看看帮助，我们发现了我们可以自定义开始寻找的下标和寻找结束的下标。我们从html里面发现我们想要爬取的链接相隔都不是很远，都处在同一个div下面。于是我们来试试，从第一条链接后面开始寻找第二条链接。 这里我们要注意后面两条代码，我们选择了开始的下标是从上一条链接的尾部开始的。现在我们来试试是否可以获取正确的链接。 现在我们获取到了三条不同的链接，我们再通过对比html来看看我是否获取的是正确的链接。 从结果来看，我们的代码成功的获取了本页的前几篇文章的链接。关于怎么获取剩下的链接我们应该有头绪了。当然，这里一页只有10篇文章，也就是只有10个链接，我们可以把我们的获取链接的代码复制10次，可是如果一页有20篇，30篇，50甚至是100篇呢，难道我们也要将代码复制那么多的次数，肯定不能，也不科学。很多人现在已经知道要用循环来做了，但是这个要怎么循环，从哪里循环呢？我们再来看看我们上面的代码，我们发现除了第一条链接获取的代码不一样，后面两条链接获取的代码都是一样的，这时我们就知道我们应该从第二条链接获取代码进行循环了。 这里我们要先定义一个列表对获取的链接进行存储，因为是10篇文章，所以这里定义的就是一个10个元素的空的字符串列表。下面是我们循环的代码块。 这里结束一下我们为什么不是从0开始进行赋值，大家注意到没有，我们是从第二条文章链接看是循环的，那么第一条的文章链接在哪呢？当然是存储在了列表的第一个位置，也就是下标为0的那个位置了，关于range后面的范围，大家知道是包下不包上的就行了，就是说在range（x，y）的循环中，循环是从x开始，到y-1结束的，不包括y本身。我们现在来运行一下我们的代码看看是否获取的是正确的链接。 然后再次对比html 这时我们发现我们获取了链接是正确的，那么我们就要开始进行下载了。还是上一节课的代码，不过我们进行修改一些地方。因为上次只是单个链接，这次我们有一个链接列表，所以我们应该采取循环进行下载。我们要对下载重新写一个循环了。 ) 我们现在来试试，这是上一节课我们成功下载的第一篇文章。 我们现在删掉他。 现在我们看到文件夹里面是什么都没有的，我们现在开始下载。 我们这就下载完了，我打开其中一个看看。还是注意地址栏上面的链接。 可以看出我们获取的文章是正确的。因为篇幅有限，我就不一个一个去打开截图了，大家自己可以根据自己实际环境敲一下代码。","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"python,爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"三个步骤搞定一个爬虫（1）","slug":"三个步骤搞定一个爬虫（1）","date":"2017-11-18T14:57:45.000Z","updated":"2017-11-19T03:47:50.383Z","comments":true,"path":"2017/11/18/三个步骤搞定一个爬虫（1）/","link":"","permalink":"http://damiantuan.xyz/2017/11/18/三个步骤搞定一个爬虫（1）/","excerpt":"这篇文章算是我刚刚开始学爬虫的时候写的一个笔记了，现在整理出来，也算是一个分享过程吧。爬虫总体归纳来说就是三个步骤，获取网页源码，定位信息，提取保存信息，我们一个一个来说。","text":"这篇文章算是我刚刚开始学爬虫的时候写的一个笔记了，现在整理出来，也算是一个分享过程吧。爬虫总体归纳来说就是三个步骤，获取网页源码，定位信息，提取保存信息，我们一个一个来说。 什么是爬虫 我们都用过搜索引擎，嗯，就是Google，百度，必应之类的东西，我们为什么可以在上面搜索出东西来就是因为他们有爬虫程序在后台帮他们收集数据。就像我们要从网上获取数据我们就要去浏览网页，爬虫就是充当了一个浏览网页的机器人，将获取到的信息返回给自己的服务器。 目标 从网页上获取一篇文章 思路 其实我们不必对爬虫感到很神秘，毕竟我们每天都在用的东西。爬虫其实本质上就是在模拟用户的浏览行为，只要我们抓住这一点的话我们就可以围绕这点来展开了。用户浏览网页首先是要打开网页，再从网页上面获取到自己需要的信息，我们可以用爬虫来完成这一行为。 方法 尝试获取到网页的源码，再从中提取数据。 准备工作 因为本人一直对推理悬疑比较感兴趣，所以这次爬取的网站也是平时看一些悬疑故事的网站，同时也是因为这个网站在编码上面和一些大网站的博客不同，并不那么规范，所以对于初学者还是有一定的挑战性的。演示系统用的是kali，因为懒得去配置各种py模块了，就利用系统已经配置好的，浏览器是firefox，使用的IDE是微软的vscode。python版本为2.7 查看robots.txt 首先我们选取了我们要爬取的网站http://tuilixue.com/，先检查一下robots.txt看看是否存在有一些反爬虫的信息。 很好，这里没有什么限制 查看目标 然后我们到我平时比较常去的板块看看，http://tuilixue.com/zhentantuilizhishi/list_4_1.html我们现在想要爬取的文章就是这样的 右击鼠标查看源代码，我们可以看到，我们想要爬取的链接就是这样的 来一张清晰的 开始工作获取网页源码 但是我们要怎么办才能使python得到这个网页的源代码呢？我们可以使用python的urllib模块提供的open方法，首先我们先新建一个py文件，惯例12#-*-coding:utf8-*-#!usr/bin/python 因为是linux系统，所以python路径不同于windows，第一行代码说明是用的uft-8进行编码 在这里我们要先导入urllib这个模块，使用import导入。这里其实是两个方法，一个open一个read，open用于从网站上获取网页代码，read是为了读出来好打印。 我们可以得到上面结果，但是我们发现字符似乎成了乱码，为了找到原因，我们再来看看源码。 我们似乎找到了原因，网页使用的是gb2312进行编码的，但是我们是使用utf-8的，所以导致的乱码，对这方面不解的同学可以去找一些编码的知识看看。下面我们用一个编码转换来尝试获取正确的编码。 这时可以看到，我们通过强制的编码将获取的网页重新通过gb2312进行编码，我们就可以看到正确的字符了，但是在我们的这次课中并不需要这样的转码，这里只是为了显示获取的是正确的网页，从图中看到，我们获取的正是我们需要进行爬取的页面。 分析源码 下一步，我们需要获取我们本页的所有的文章链接了，这里需要有一点html和css的知识，关于这部分的知识，大家自己去掌握就行了，不需要太深入。如图中显示的，href后面的就是我们在本次课中需要爬取的链接，每页都有10篇文章是我们需要爬取的，我们先从第一篇的链接开始。 这时候我们就要想我们应该怎么样去获取到这个页面的链接了，如果正则表达式好的同学应该是想到了采取正则表达式进行获取，但是这里有一个问题，一个html页面中有如此多的a开头的元素，也有如此多的href开头的元素，想要通过正则去定位还是有点难的，就算定位出来，也是一大堆的代码，这就不利于可读性了。这时我们应该再从html文本中去分析。我们使用type函数进行类型的判断。 通过对pageContent的类型分析，我们知道这是一个字符串类型，这样我们就可以使用字符串中的find函数了，我们需要对find函数有一个了解。 函数中说明了从字符串中寻找目标字符，返回找到的第一个下标，如果没有找到就返回-1,同时可以设置开始寻找的位置和结束的位置。我们再看到文本。 我们发现是在div class=“liszw”下的li元素中的a元素中含有我们需要的链接，这时我们一个个来分析。 这时我们发现这和我们所要爬取的链接数量上是完全吻合的。我们就来试试。 这里我们采取了一个切片操作，这时我们发现链接其实已经爬取到了，但是还是有些不完美，我们再来完善一下他。 我们来对比一下我们的网页上的第一个链接 保存信息 这样我们就成功的爬取了第一个链接，现在我们来准备下载第一篇文章。从前面我们可以知道，我们可以把网页通过python的urllib模块下载下来，那么同样的道理，我一样也可以通过urllib模块对文章进行下载。我们通过链接的最后一串数字对下载下来的文件进行命名。并在下载玩后打印end进行提示。 我们可以看到，路径下是没有文件的。现在我们开始下载。 从这里看我们的文件应该是下载成功了，我们去文件路径下面看看。 文件下载是成功了，我们来打开看看。这个地方要注意地址栏的链接 这样，通过三个步骤，我们的爬虫就已经完成了。","categories":[{"name":"python爬虫","slug":"python爬虫","permalink":"http://damiantuan.xyz/categories/python爬虫/"}],"tags":[{"name":"python,爬虫","slug":"python-爬虫","permalink":"http://damiantuan.xyz/tags/python-爬虫/"}]},{"title":"我的第一篇文章","slug":"我的第一篇文章","date":"2017-11-18T06:09:13.000Z","updated":"2017-11-18T07:54:11.772Z","comments":true,"path":"2017/11/18/我的第一篇文章/","link":"","permalink":"http://damiantuan.xyz/2017/11/18/我的第一篇文章/","excerpt":"其实第一篇文章也不知道说什么好，简单的就说说搞博客的这件事吧。","text":"其实第一篇文章也不知道说什么好，简单的就说说搞博客的这件事吧。 第一篇文章 其实第一篇文章也不知道说什么好，之前对前端的知识只是掌握了一点HTML+CSS，其他的不会太多，也没有用Markdown语法写过什么东西，这是第一次尝试，这第一篇文章就当作是一个test吧，因为我也不知道有什么效果。因为第一篇文章，我也想随便写点什么东西，简单的就说说搞博客的这件事吧。 来说说为什么开个博客 这个问题其实也很好解释，朋友圈已经沦陷了，成了微商和投票专用的通告栏，还成为了七大姑八大姨八卦的云聊天室，所以个人微信上已经不怎么用了。关于QQ空间这个其实和微信的朋友圈差不多，成了卖东西的地方和鸡汤遍地之所，而且朋友们很多都已经不玩QQ空间了，而且空间也并不是一个开放的地方，同时很多敏感的代码会被和谐掉，所以形同鸡肋。而且朋友中搞这个的人并不太多，交流还是少的。 关于hexo hexo的好处有很多，什么简单啊，可以免费使用github的空间之类的，这些文章网上有很多了，一搜就能够搜到一大堆，其中不乏大牛的源码分析，我也写不出什么东西来。只能说选这个静态博客框架纯粹是因为自己懒，不想去搞服务器也不想去配置什么东西了，而且别到时候没有续费就把东西全部清理掉了那才是坑，也不想到别的什么博客平台去注册什么账号的，因为很有可能自己写的文章就突然间不见了，就想着简简单单的写点东西，第一个算是自己在学习的路上做个笔记，第二个也是可以分享出来给大家看，技术这点东西其实没什么可以隐瞒的，不过你学得比人家多你就知道一些人家暂时还不知道的东西而已，多分享一下自己踩过的坑，后面的人的路就会顺一点。同时也是督促自己要去学习吧，毕竟博客总不更新会觉得怪怪的，说我好学也好，装哔也罢，不动脑不动笔写点东西，整个人都快生锈了。 关于博客 开个博客总要写点什么吧，其实我也不太清楚最后这个博客会变成什么样子，慢慢写的，自己学到什么东西就写点什么东西吧，不过总的来说应该都是python方面的学习笔记，偶尔会有点经济学，经济行为学和心理学与管理学方面的文章穿插进来。就这样吧，像背景音乐中的一句，就老去吧，孤独别醒来，你渴望的离开，只是无处停摆。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://damiantuan.xyz/categories/随笔/"}],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://damiantuan.xyz/tags/随笔/"}]}]}